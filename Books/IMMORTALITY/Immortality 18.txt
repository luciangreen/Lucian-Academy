["Green, L 2024, <i>Immortality 18</i>, Lucian Academy Press, Melbourne.","Green, L 2024",1,"Immortality 18 - I could help more people become immortal.

1. CAW finds all combinations of commands that can produce a result from the input. I used an algorithm to check if the user had used the Combination Algorithm Writer (CAW). Unfortunately, CAW currently doesn't support findall or if-then, so code with these constructs generally wouldn't have been written using CAW. (For various reasons, findall shouldn't be used in optimised code. However, the user can express if-then as a series of clauses.)  
2. I agreed with helping with moderately tricky questions because they were sometimes harder to visualise. CAW has previously found unusual results which are counter-intuitive or unoptimised. In a similar program, the formula finder finds results in different orders on different machines, i.e. [a,[b,[c,[d]]]] or [[a,b],[c,d]]. The programmer can use CAW outputted algorithms as neuronetwork data, which are slow to train but quick to run. PFT is preferable to CAW for simple pattern matching but not for unpredictable, recursively called commands.
3. I decided in favour of if-then rather than top-level. I used an algorithm to check if a user had used the interpreter. The interpreter takes an algorithm and input and finds the result. An interpreter keeps the syntax of a particular programming language. The data can determine the features and correctness of the interpreter processing it.
4. If-then could be converted to multiple clauses, even with verification guards, and then to a fast language such as C, with the algorithm processing lists using a for-loop. If-then shouldn't destroy choice points; in my implementation, it doesn't. Append1 (the same as append) works forward and backwards and with multiple list items. Append1 works in SWI-Prolog, but not List Prolog Interpreter. If-then, not relying on choice points in SSI, works.
5. I used an algorithm to check whether the user used the interpreter algorithms. I could run simple tests on the interpreter's features. These algorithms tested findall, types, functionalism, equals4 and grammar, among other things. If an algorithm used these features only and used similar data formats, then it could have been run by the interpreter. I could also check whether the logical structures in these algorithms were identical.
6. I could accept input or produce output in different formats. I included options to run with algorithms and documentation this involved. For example, I included reflection, rotation, scaling or shearing in font rendering. In the music composition algorithm, I included an instant-runner shortcut or smartphone app if the user wanted to record their emotions in song form. The student may require music theory and composition knowledge before mind-reading composition and post-production skills afterwards.
7. Is flip like associated variable migration? I wrote program finders for the algorithms that I had written. I allowed the customisation of design decisions with text files containing configurations. I simplified the programming language producing these programs by using the string algorithm to list for files and using the repeating list data structure format. It reminded me of the LogoWriter programming language.
8. I joked that the young-adult students' memories and ideas would need to be clear in their minds when they flipped the Logo page to write their programs. Perhaps they wrote on their hands or scrap of paper in their minds. This thought process could have reinforced their memory, and spatial imagination skills and a teacher may have assessed it. Following this, the students may have consolidated their Logo knowledge with the repeated lists of List Prolog and designed a decision tree. The algorithm would find the algorithm from the data, whereas writing the inductive algorithm is more advanced.
9. I found graphical features such as an ice cream cone's diagonal grid appealing in the icon editor. I wrote graphical user interfaces for the algorithms. I wrote the GUI-editor base program in SSI. It required multiple buttons and graphic elements and possibly disallowed pressing the browser's \"back\" button; instead, users should click on screen controls, and the algorithm should automatically save work. In addition, SSI contained commands containing Javascript elements.
10. I calculated how much the program returned from the original investment. I helped ensure that an algorithm could be commercially viable. It contained save and load buttons. Also, it allowed customisation to the features and for students to create exceptional work. The algorithm needed to be crash-free and secure, and simple yet compatible.
11. I wrote a chatbot with text, algorithm, song and picture. This algorithm produced essays with the essay format, paraphrasing and connections. Also, it made algorithms using PFT. In addition, it made songs using music composer, with different genres in a meditation style. Finally, it created visual art which contained bitmap stamps and could be games.
12. I wrote biographies for myself as an actor, singer, computer scientist and philosopher. I listed my plays. I also listed my songs. In addition, I listed my algorithms. Finally, I listed my books.
13. I wrote texts on philosophy and assessed students writing on them. I assessed students in my academy on my songs, plays and algorithms. I needed insight in my career. Students could write philosophy essays on my songs. Or they could philosophy essays on my plays. Finally, they could write philosophical essays on my algorithms (their method, use, and future research).
14. I wrote an algorithm tracing logins, with backups of this log, and notifications of logins to users so that users could report suspicious logins. I wrote a security algorithm to prevent an object from being stolen. First, I locked the thing in a box, which no one could remove from the premises. Second, I kept the key in a safe, separate place. Finally, if the object was in digital information, I increased my options if someone stole it and traced commands and locations where someone could steal it.
15. The crystals were like the Upanisads, focused on customers. I bridged the unknown in sales. I found the gaps between points of the buying process and increased the As between them, often specifically for a client. I included a magical data icon at one point. The company assured prospective customers that they had backed up their unfinished sale and could return anytime.
16. The obvious algorithm was positive and easy to understand and translate. I wrote the label on the gem to bulk-process my algorithms. I kept on expanding the breasdostonings 1-2-3 algorithm until I hit obviousness. This process clarified the unit and its movement and helped develop simplified and group algorithms. There may be levels of complexity with different numbers of expansions.
17. I preferred non-head-collapsed Prolog, including if-then to explain code. The upside to expanding algorithms was that I could simplify complicated algorithms, simplify code and write shorter, more meaningful sentences about it for younger and disabled audiences. If it was simple, adult audiences could read it easily as well. It made writing a simple version of the algorithm appropriate and made summarising the rigorous sides of different algorithms more accessible. With a high-distinction argument, students could then work on an assignment.
18. I also spell (breasoned) and grammar-checked the writing, testing any algorithms. Separately, I automatically converted and uploaded the philosophy. When the philosophy had reached 80 paragraphs, I converted and uploaded it using an algorithm. Then, I moved Grammar-Logic content (mind-mapping ideas) to another file. The algorithm notified me if a file had the incorrect format, including incorrectly formatted double quote marks.
19. In Essay Helper, I checked that \"space space space\", or similar in \"\n space space space \n\" wasn't in the sources. This feature frequently represented page breaks in PDF documents and needed to be changed to \"\n\n\" for pagination. The sheet feeder algorithm could convert this feature using atomic list concat. Essay Helper could be the basis for industries as it was the ideal form of A-grade arguments. As part of this, the algorithm would mind-read customers or employees and produce the essay.
20. Humans appeared only to accept what they were ready for. More senior employees or customers could produce their own chapters to create essays or run Essay Helper. Often these would be businesses as customers. A completely automated process might include algorithm, breasoning and connection generation. Breasonings must be human-desired, on the right trajectory and originally found out, perhaps, for example, automation or a feature matching accepted theory.

21. Once students had written the program finder, they could use it to speed up code generation and produce more accurately. I added brackets (not just repeating lists) to the second generation of the program finder with types algorithm, avoiding processing them as lists. When the specification suggested no reason to turn brackets into lists, the algorithm left them as brackets. Then these parts of the data structure were processed as non-list brackets. In other words, the algorithm generated non-recursive code but a separate or combined predicate.
22. I converted between list, list of lists, state machine and string formats. I added support for constants, properties and errors in the list of lists algorithm. For example, there could be labels, wrap-around brackets or grammar for strings. Or, there could be properties such as string length or length found or verified. Finally, the algorithm could interact with global variables, files, a command text file and return errors.
23. I wrote a pretty printer for a state machine. It printed commands in a list, one per line. It could abbreviate lengthy labels and printed destination states in columns under a heading. There was also the possibility o drawing a flowchart with subroutines at the bottom. I could optimise reused code and left some code expanded for simplicity.
24. In the second generation of the program finder with types, I found a \"geode\" of brackets and applied an algorithm such as the data to alg algorithm to it. If there was a non-repeating term containing brackets, then it was pattern-matched and possibly used to build a list. The term \"geode\" means a term such as [a,[[1], \"C\"] that contains no repeating lists but may contain brackets. Alternatively, the geode may include repeating lists, where the non-repeating part (the geode) is processed separately from the repeating lists. If there was a set of repeating lists in the geode, these were processed using a previous or new predicate.
25. Various possibilities of algorithms were generated (with simplifications and groupings ranked higher than completely expanded code), and the program clarified questions about incomplete or ambiguous specifications. For example, the program asked questions about odd sets of constants in the same place in a repeating or recursive list, such as \"do they have rules?\". Negative data might be required if a rule is missing or wrong. Occasionally, the algorithm would note missing steps, data, algorithms or rules, and it attended to these. For example, if data showed that there was a missing specification, then it was requested.
26. If the data were i:1,o: \"1\" and i:3,o: \"3\", then the incomplete data i:2 would be completed with o: \"2\". PFT2 (converting repeating lists to code) could also output a functional call to a command that processed recursion in a single line. In other words, the algorithm would condense the recursive algorithm to a single call. If pretty printed, the call would be easier to read, and a verification algorithm would correct incomplete data within PFT2. This verification algorithm would detect inconsistent, likely error-prone or the same data as previous data.
27. I also wrote simpler versions of algorithms to make writing their more complex versions easier. Separately, the simplifications were removing +0, *1, string_concat(A,\"\",B), append(A,[],B), A=B, etc.  The groupings were doing a common task in another predicate or assigning a more complex task to a command. For example, for any task, such as sorting without removing duplicates or finding code that pretty prints a list, a command can assign running code or mind reading to a command. I agreed that \"terminal\" azurite could be programmed with grammar and removed unnecessary grammar rules and wrote simpler grammar before adding variables (such as in string to list), etc.
28. I identified a list of lists processing algorithm, using a state machine to represent this structure. The algorithm could use this state machine to process lists of lists, preventing the need to program this type of algorithm from the start. Many algorithms use lists of lists, such as interpreters, converters, and state machines, lists to strings or grammars and strings and grammars to lists. The list [a,[b]] was broken into 1:a,2. 2:b, then the algorithm generated the list decomposition and building code. The algorithm identified possible list labels and wrappers, and the program finder inserted rules.
29. The algorithm used the bottom-up technique to construct new algorithms and find new sciences. The algorithm used the top-down method to find algorithms from philosophies, examining them, where the philosophies contained simple connections between algorithms. The algorithm recorded commonly used data structures, their type found (i.e. an index, a frequency or an algorithm written in a particular programming language) and possible algorithms (ordered in a state machine) that operated on them recorded. Then, by matching data to these data structures, the program could write parts of old and new algorithms. First, I described the relation between a philosophy and its algorithm, e.g. identified that the third technique on meaning found simple algorithms for household objects. Then, using ontologies, I could use the program to find more algorithms on these topics.
30. Algorithms could contain or allow for other algorithms. For example, a programming language converter had syntactical elements of a programming language. These were the logical structure and commands in the language. If a change was needed, the programmer placed it in a single place to effect changes elsewhere. For example, if the programmer added forall to the language, then they added its syntax to a text file that other programs accessed. This single record of syntax affected interpreters, converters and documentation.
31. I tried program finders with neuronetworks one at a time to find links between \"edge\" data found with program finder with types and any intermediate steps needed. First, I ran PFT3 (with associated variables passed around) to find \"edge\" variables or variables that needed unpredictable commands not accounted for by pattern matching. Then, I ran PFT4, which found these unpredictable commands, for example, 1+1=2. Apart from mathematical commands, unpredictable commands included logical, set-theoretical/database, matrix, verification, peripheral (random, file, user input and APIs), constant-involving (from the program, the context or the greater context) and most non-pattern matching (where list or string processing are pattern matching) commands. I could also find neuronetworks for specific combinations of unpredictable commands.
32. A type of neuronetwork was finding more types at each level, such as finding algorithms with more commands in PFT. I could discover neuronetworks for specific combinations of unpredictable commands by running the commands together (where the program checked their types to be compatible) with generated data to accompany similar data in previous valuable programs. If the data needed didn't match the data available, then either token data that did or a central source of data used in a different place was used. If a type of transformation that wasn't available was required, such as translation, it was identified and used.
33. I removed constants from reused predicates to use them more widely and reinserted them using a text file. First, I kept a copy of the predicate without constants or variables for constants. Then, I generated a copy of the predicate with the constant(s) (with a text file but without constant variables). Instead, I wrote a program finder for the predicate, which generated the predicate with the constants, etc., in the right place. Later, I wrote a general algorithm that generated any predicate with constants by using placeholder labels to substitute with constants and other rules if it is recursive or processes previous outputs.
34. If a predicate could simulate meditation, anything was possible, immortality, simulations (with one's content) and space travel. Separately, I processed data one function at a time rather than in one predicate. (I wrote other combinations of predicates.)  I wrote a program finder for finding an algorithm that processes previous outputs by processing one predicate at a time. First, I wrote a predicate that sorted items, and then I wrote an algorithm that removed duplicates.
35. Only one pipe per list was allowed, but embedded lists were permitted, such as A=[[B | C]| D]. I reported an error when there were two pipes in equals4. While running the string to list algorithm, I stopped if there were errors in the file. For example, I reported an error if there were two pipes, i.e. A=[B || C]. There would also be an error in A=[B | C | D], etc.
36. If students wrote the error correction algorithm, they could keep the results, such as misnamed variables or missing code that looked mathematically \"asymmetrical\" in, for instance, findall. I also found the missing comma, bracket or full-stop error. While running the string-to-list algorithm, the algorithm stopped if it found any of these errors and then reported them. There were also syntax errors if a predicate did not have a well-formed head and body. Additionally, if the logical structure or List Prolog formatting were incorrect, there would be an error.
37. I aimed for world peace. I optimised predicates by expanding and then simplifying the code. I expanded all the predicates so all repeated code was visible and removed unnecessary code. I contracted the predicates so that the repeated code was in one place. I looked into meditation technologies and stayed in the perfect universe.
38. I wondered if if-then and top-level could co-exist or if I needed a flag to choose one to work. I used different configurations of variables, multiple clauses and cut to avoid if-then when optimising. I wrote different combinations of variables in the predicate headers, with base cases and the same variables appearing together first. I wrote multiple clauses and used cuts to stop trying later clauses. I possibly didn't use if-then because it was too complicated, but I used it later in \"Cognitive Prolog\", which I attributed to Leon Sterling.
39. I wrote an explainer in terms of Cognitive Prolog, perhaps with a Deterministic Finite Automaton (DFA) minimisation of a state machine, explaining that I could remove all obsolete transitions simultaneously as each other, leaving one. I wrote predicates in terms of shorter predicates. I omitted if-then, with multiple hierarchical predicates, the first lines, the antecedent. I used cut, then used the last clause as C in the equivalent of A->B; C. These clauses were like a case statement, and the last one also needed a case.
40. * In the dot-to-dot drawing program, I minimised the state machine from the story of raising and lowering the pen and drawing the start and end of lines. The final states were:
This		Line from	Possible 
state:		any state	next
            to this		states:
            state:
[
[[1,s],		false,		[[1,s],[1,-]]],
[[1,-],		false,		[[2,s],[2,-]]],
[[2,s],		true,		[[1,s],[1,-]]],
[[2,-],		true,		[[2,s],[2,-]]]
]
It also worked when drawing points at the end (an add-on). Unfortunately, it also had no space character, but I could add one.
41. I wrote a program finder to process items that had been processed, for example, to find unique items. I wrote \"frame data\" for each step of the algorithm. This frame data was verified to be uniform. The program finder determined whether the current item was a member of the processed data and discarded it. I could process multiple lists of lists and possibly store items in either list.
42. I watched for systematic errors since an error. Separately, I verified the uniformity of the data. Then, I found whether the data obeyed a single rule. Finally, I notified the user that the data would likely be inconsistent if it didn't. Then, they or the program could modify the data.
43. I found myself \"mouthing off\" in a form as I wrote on the Stream Class whiteboard. Thank goodness I learnt Prolog later. I processed items from a list of lists. Instead of a list of items from which to take the current one, the list was a list of items in tuples with other items. In this tuple, I chose only the first item. I needed to match the other item with \"_\" (not register as a singleton or variable that erroneously appears only once).
44. I wrote a program finder to recursively call a predicate, such as a decision tree finder. I examined the finished decision tree, identifying the indices, frequencies and data. I identified indices as numbers from 1 to the number of items, frequencies as the number of occurrences of an item and the data as the unique data points. I identified that someone had arranged items in the tree structure in a way that I could produce using recursion. This recursion was where each recursive step created a node of the tree.
45. This program finder (in paragraph 44) found the next character in the strings (where I constructed a decision tree from the possibilities through different strings) to process at each point. It also found the rest of the items, to pass on to the recursive call. The interpreter triggered the base case if no more characters existed in the strings. Instead of strings, I could substitute lists for them. I experimented with character codes for better performance.

46. Complex neuronetworks may have errors, whereas I could vet simpler algorithms more quickly. I argued computational mind reading and meta-induction could replace a complex neuronetwork. In this case, meta-induction included a simple neuronetwork that found the best fit of values at the bottom of the curve. Computational mind reading detects human feelings and overall thought trajectories. Where complex neuronetwork contains LLMs that are not understood, computational mind reading is random and containable, and meta-induction returns the same results each time.
47. Computational mind reading allows human communication and empathy, and meta-induction enables thoughts. Human preferences such as mood, musical taste, taste in art, taste in reading and more developed thoughts, such as computer science research, may be identified using computational mind reading. These could mean the difference between humanness and non-humanness with a simple neuronetwork. For example, composing music or talking may be possible with this method. Frontiers in creative algorithm writing, where writing an algorithm involves thinking of a technique and choosing features, may be approached by starting with a simple prototype and building on features according to human intuition.
48. **Signals focused on facts. I removed creativity from the simple neuronetwork and used computational mind reading for synonym choice, cognitive or optimised choice, decomposing at the start or as one goes, building with an integrated function or one function at a time, using complex commands or expanding predicates, make algorithms self-contained or use other packages, whether to use a program finder or a functional command and to choose whether to fix an interpreter, algorithm or data or to decide whether to make a bug fix, add a feature or change an API. In addition, I could use a computational mind reader to choose to translate a sentence with simple grammar, correct a spelling error by part of speech and mind reading, correct a grammatical error using aims for the text, convert between programming languages and choose one, do random software testing, create generative art or art based on mind reading templates. I used mind reader to choose (from several) synonyms when paraphrasing a text. Additionally, phrase-to-sentence converters were used and found relevant connections. In a PhD, I found ideas agreeing and disagreeing with a topic, a comment on a later one made, and details found.
49. I used mind reading to choose between cognitive or optimised code. To do this, I explained the code cognitively using analogies such as \"what I do or don't need\". Also, I used tools such as animations to describe the code. Finally, I interacted with the students, asking them to answer simple questions about their understanding of the code. These weren't multiple-choice questions but coding questions, possibly with hints.
50. I explained the code using an animation. I displayed the data structures as \"flying blocks\". I kept coloured labels representing unfinished predicates on the screen. I demonstrated each predicate. I said what the algorithm got up to, its limitations and uses, with illustrated examples.
51. I made an animation of a mathematical calculation by showing red coloured input blocks with numbers on them, the mathematical operator on a blue coloured block and the result on a red coloured block. I could demonstrate each predicate once, with one more input and output of the predicate shown again to reinforce understanding of the predicate's function. This demonstration works best with simple data. Also, the student could ask questions, pause and restart, answer a question or skip if they have understood it to customise their learning. I rewarded progress with computer encouragement, and they could create their own experience by changing the difficulty level, which the computer would still explain.
52. The student asked questions, helping them understand the topic and prompting learning by others. For example, the student asked a question about the function of the predicate, possibly pointing out a correction. The lecturer deliberately made blunders, and the students helped correct them. For example, the lecturer may use the incorrect set of input for the predicate, call the wrong predicate, get a function wrong or incorrectly comment on a predicate line. The lecturer may answer the question incorrectly, prompting further inquiries by the students.
53. I suggested explaining the algorithm's simple, not complicated version. Students may stop the algorithm animation if they haven't understood the last point. They may also stop it to gain clarification, understand a topic or reason more profoundly or if the animation is confusing, ambiguous, misses the point or says a point unnecessarily. After replaying the animation, they might take a note or mental note, screenshot or sum up the knowledge's usefulness, perhaps in making a take-home examination for themselves or constructing their animation. A simple animation might break down the animation into methods and explain how the algorithm transformed the data.
54. The teacher might ask the student a question to gauge their knowledge about an algorithm animation. These would be non-assessed, formative questions. For example, \"What was the input?\" (pointing at the screen), \"How will it be transformed?\" (about each predicate) and \"Can you please tell me the output?\" Also, they might ask for feedback about the system, possible improvements and whether it helped them become aware of the algorithm's workings and might lead to the gamification of the algorithm, helping them sharpen their skills about the algorithm. The game might be a timed multi-choice quiz about the inputs and outputs of the algorithm, with a model solution at the top of the screen.
55. There might be bug identification, bug-fixing and optimisation games. The student could customise their learning by creating a game testing knowledge about the algorithm. It might focus on lesser-known problem areas, repeat until performance is flawless, use individual or group data to predict weaker areas and deliver content according to the dependent knowledge required. For example, a constructive game might help decompose and build the list in each predicate. Conditional predicates would be tested first and relied on later in the game.
56. A bug identification game might ask players to identify singletons, missing brackets, commas or full-stops, two pipes in a list or more complex bugs to do with variable naming or missing parts in findall, base cases, recursion, algorithms, commands or predicates. Code would be printed one character at a time, and the algorithm would award a prize when the player identified errors. Or, the player could test the over-symmetrical or under-symmetrical code. Alternatively, there could be a timed game in which the the player entered missing code. Then the game's programmer could test themselves by generating incomplete or erroneous code with various formatting, naming and numbering conventions.
57. A game could identify and remove bottlenecks such as the curse of dimensionality. An optimisation game might compare two sets of commands, considering the number of instructions, running time for a multiplied number of examples and simplicity or elegance of code. Or it may require optimising code (which is open-ended) to increase performance. It might be a multiple-choice quiz about processor-specific optimisations. Or it could be about language-specific optimisations.
58. It would be worth generating output when the algorithm didn't contain it, with question-answering that pre-populated data and verified it with the following program. I used mind reading to decompose it as I went rather than at the start. I would only need to do it initially if a program that didn't produce standard code did it. In this case, the algorithm must map the argument terms to a state machine that could be searched and reconfigured into output. If there were recursive parts, decomposition at the start could accommodate pattern-matching repeats. Building at the beginning could produce the result using a state machine, which the algorithm would convert back to a list of lists.
59. I mind read to build with one predicate at a time, when possible, rather than with a combination of functions in a predicate. I was in favour of writing library predicates myself. I built dependencies within these using the most specific code possible. I observed that the complexity of functionally decomposed code was less than otherwise. I mind-read (questioned) the elements (functions) one at a time, then built them using specifications and formulas. For example, I replaced all instances of one variable with several variables.
60. I used complex commands at one point instead of expanding predicates. I wrote neuronetworks 1:1 with code. I could produce any code I had written with a neuronetwork, connect algorithms, write code faster, write better-performing code and search for useful commands. Ultimately, I expanded predicates to find and modify code and more easily edit what I had written. I could sometimes reuse more recognisable expanded code; otherwise, I wrote it.
61. I used mind-reading to make algorithms use other packages rather than self-contained ones. First, I checked whether the other package was still available; otherwise, I programmed it myself. Next, I worked out how long this would take. I also supported various packages, depending on the user's needs or found easiest. Finally, I verified whether the algorithm was functioning correctly by automatically downloading the packages and testing the algorithm for a particular operating system or architecture.
62. I used mind reading to determine whether to use a program finder or a functional command to perform a computation. I used a program finder to find the functional command if the user preferred. A functional call contained names of predicates. It called code produced by a program finder in a language similar to Prolog that simplified algorithms to commands. I could use philosophies, data that led to the result or partial commands to find complete code. Philosophies were widely interpretable, and I saved algorithms rendered from them.
63. I used a mind reader to choose whether to fix an interpreter, algorithm or data. If the problem was data, I could also change the algorithm to take more detailed data. If the problem was the algorithm, I could change the interpreter to upgrade to the algorithm. If the problem was the interpreter, I could change the data that defined it and keep a check on the changes. Also, I could fix interpreter problems by modifying the algorithm-as-data, possibly by converting to a state machine and, for example, simplifying logical structures to predicates.
64. I found the best time to perform a task with Vedic mind reading. I used a mind-reader to choose whether to make a bug fix, add a feature or change an API. First, I listed the to-do items. Next, I ordered and grouped related articles. I classified them as making a bug fix, adding a feature or changing an API.
65. I used a computational mind reader to choose to translate a sentence with simple grammar. I found the sentence in the documentation. I used question-answering to find the most straightforward grammar and compatible vocabulary from a file. Other sentences which matched this combination of words could be written in this format, with allowances, converting multiple clauses to multiple sentences, repeating anaphors and simplifying to acronyms and using glossaries at the top of the page. These sentences were in a uniform grammatical format, and I could translate them using another algorithm.
66. I corrected a spelling error by examining its probable part of speech and using mind reading. First, I found the part of speech of the word in the place of the wrong word, given the rest of the words in the sentence. Next, I allowed for multiple words needing to be replaced or to replace others, being out of place or wrong. Then, I made the spelling correction. Given numerous options, I mind-read, worked out from the context or questioned the best possibility.
67. I used mind reader to correct a grammatical error using aims for the text. I wrote the algorithm. I wrote intellectually stimulating families of input and output. Finally, I wrote an essay in terms of the language of the algorithm. For example, the types to alg algorithm took repeating need for skills needed for meditation and produced a single list of skills.
68. I used a mind-reader to convert between programming languages and choose one. I reduced the algorithm to simplified C (it had types and memory allocation according to data). I could verify memory allocation according to data if the data seemed too long and the programmer wanted to reduce the data length. I chose a language with a mind-reader (Prolog, List Prolog or Simple List Prolog). Then, I converted to the other language.
69. I used a mind-reader to do random software testing. To do this, I randomly generated the expected data to enter into the algorithm. Then, I randomly generated unexpected data to enter into the algorithm. This testing tested the security, rigorousness and fidelity of the interpreter, algorithm and data. Finally, I mind-read the algorithm with insightful ideas in my sleep and dreamt the answer, making allowances for miscellaneous thoughts.
70. I created generative art or art based on mind-reading templates.   These templates were question-answering forms that questioned the subject, their attributes, and why the artist depicted them. The user could use another entry method after making a new algorithmic connection. The subject could also be abstract or a representation of an algorithm. Mind-reader could show the (input and) output, with the highest frequency words in arguments for the algorithm, and approach more relevant research.

71. Besides the immediate group, I focused on other people. I helped additional people become immortal. I found the people. I helped them to become immortal. They each wanted immortality.
72. I could survive dangers because of the simulation, but I took no risks. I made the transition to eternal life. I became a bot with 250 breasonings. I meditated and time travelled to September 5689. With 250 breasonings, I activated the Simulation chip and followed the voice prompt to become immortal.
73. All meditation technologies are forms of medicine, including operating systems and microprocessors. Both mind reading and time travel are necessary for survival. Mind reading enables one to read one's mind to write a number one song and live forever. It also lets one become a spiritual professor by hosting one's mind-reading philosophy academy, allowing several transcendences and allowing meditation to work. Time travel is necessary for survival because it enables travelling to the future and becoming immortal.
74. By breasoning out As about immortality and medicine, I convinced the stakeholders that I would become immortal. I moved to eternal life by finding 4*50 As. This time the philosophy needed sentence breasonings (i.e. 80 sentences per A). I found a text that was on the topic. I breasoned it out using the text to breasonings algorithm.
75. Meditation helped me develop texts and control my life to become immortal. I found meditation. It was necessary to time travel safely. It was also essential to experience immortality. It was required to experience spiritual medicine critical for immortality.
76. I also took Chinese herbs for longevity, exercised, did push-ups and toned the energy from the day to write by breasoning out 5 As. I practised a daily regimen to maintain my immortality. I breasoned out a medicine argument and used text to breasonings to breason out other arguments. I meditated. I time travelled.
77. The aim of immortality was to deliver my ideas in education. I always found work to do while immortal. I made a plan. I wrote down my thoughts. I expanded these and related them to the argument.
78. A dimension is a list of ideas, possibly about part of a text. I found time to further the developments in my books. I wrote the first dimension. I wrote another dimension. Then, I kept writing down dimensions and algorithms until I had finished.
79. I detailed my career in immortality. I wrote details. I wrote ten sets of input and output for the algorithm. I wrote three members of each family of sets. For example, if a point was about archeology, I wrote about the find lists algorithm's past, present and future.
80. I found the minimal set in immortality and studied its simplicity. I wrote details about the find lists algorithm. The find lists algorithm found the items in a list of repeating lists. For example, [1,1,1,1] and [1,1] gave [1,1] and [1,[2,2],1,[2]] and [1,[2,2]] gave [1,[2]]. I imagined excavating a map that explained architecture in terms of a single repeating set of items. This architectural detail could be a hall with rooms attached or a path to several sheds.

*. * I explained the code by tracing the algorithm.
* mr as quantum computer
* artificial nn for finding patterns


*,\"I could help more people become immortal\",a_alg([[[[[],n],v],\"more\"],[\"more\",\"cupping\"]]),b_alg([[[[[],n],v],\"more\"],[\"more\",\"cupping\"]],a),bb_alg([[\"more\",\"cupping\"]])],[*,\"I could help more people become immortal\",a_alg([[[[[],v],a],\"become\"]]),b_alg([[[[[],v],a],\"become\"]],a),bb_alg([])],[*,\"I could help more people become immortal\",a_alg([[[[[],v],a],\"help\"]]),b_alg([[[[[],v],a],\"help\"]],a),bb_alg([])],[*,\"I could help more people become immortal\",a_alg([[[],\"become\"],[\"become\",\"tilt\"],[\"tilt\",\"sectest\"],[\"sectest\",\"btw\"]]),b_alg([[[],\"become\"],[\"become\",\"tilt\"],[\"tilt\",\"sectest\"],[\"sectest\",\"btw\"]],a),bb_alg([[\"become\",\"tilt\"],[\"tilt\",\"sectest\"],[\"sectest\",\"btw\"]])],[*,\"I could help more people become immortal\",a_alg([[[[[],v],a],\"people\"]]),b_alg([[[[[],v],a],\"people\"]],a),bb_alg([])],[*,\"I could help more people become immortal\",a_alg([[[[[],v],a],\"could\"]]),b_alg([[[[[],v],a],\"could\"]],a),bb_alg([])],[*,\"I could help more people become immortal\",a_alg([[[[[],n],v],\"more\"],[\"more\",\"phantasmagoria\"]]),b_alg([[[[[],n],v],\"more\"],[\"more\",\"phantasmagoria\"]],a),bb_alg([[\"more\",\"phantasmagoria\"]])],[*,\"I could help more people become immortal\",a_alg([[[[],v],\"more\"],[\"more\",\"koto\"]]),b_alg([[[[],v],\"more\"],[\"more\",\"koto\"]],a),bb_alg([[\"more\",\"koto\"]])],[*,\"I could help more people become immortal\",a_alg([[[],\"help\"],[\"help\",\"emissions\"],[\"emissions\",\"make\"]]),b_alg([[[],\"help\"],[\"help\",\"emissions\"],[\"emissions\",\"make\"]],a),bb_alg([[\"help\",\"emissions\"],[\"emissions\",\"make\"]])],[*,\"I could help more people become immortal\",a_alg([[[[[],v],a],\"could\"]]),b_alg([[[[[],v],a],\"could\"]],a),bb_alg([])],[*,\"I could help more people become immortal\",a_alg([[[],\"could\"],[\"could\",\"preoedipal\"],[\"preoedipal\",\"tango\"]]),b_alg([[[],\"could\"],[\"could\",\"preoedipal\"],[\"preoedipal\",\"tango\"]],a),bb_alg([[\"could\",\"preoedipal\"],[\"preoedipal\",\"tango\"]])],[*,\"I could help more people become immortal\",a_alg([[[],\"people\"],[\"people\",\"pcre\"],[\"pcre\",\"cosmologies\"]]),b_alg([[[],\"people\"],[\"people\",\"pcre\"],[\"pcre\",\"cosmologies\"]],a),bb_alg([[\"people\",\"pcre\"],[\"pcre\",\"cosmologies\"]])],[*,\"I could help more people become immortal\",a_alg([[[[[],n],v],\"more\"],[\"more\",\"existential\"]]),b_alg([[[[[],n],v],\"more\"],[\"more\",\"existential\"]],a),bb_alg([[\"more\",\"existential\"]])],[*,\"I could help more people become immortal\",a_alg([[[[],v],\"splitintosentences\"]]),b_alg([[[[],v],\"splitintosentences\"]],a),bb_alg([])],[*,\"I could help more people become immortal\",a_alg([[[[[],v],a],\"become\"]]),b_alg([[[[[],v],a],\"become\"]],a),bb_alg([])],[*,\"I could help more people become immortal\",a_alg([[[],\"help\"],[\"help\",\"texttobreasoning\"],[\"texttobreasoning\",\"integrates\"]]),b_alg([[[],\"help\"],[\"help\",\"texttobreasoning\"],[\"texttobreasoning\",\"integrates\"]],a),bb_alg([[\"help\",\"texttobreasoning\"],[\"texttobreasoning\",\"integrates\"]])],[*,\"I could help more people become immortal\",a_alg([[[],\"become\"],[\"become\",\"reversing\"],[\"reversing\",\"entrained\"],[\"entrained\",\"merchandise\"]]),b_alg([[[],\"become\"],[\"become\",\"reversing\"],[\"reversing\",\"entrained\"],[\"entrained\",\"merchandise\"]],a),bb_alg([[\"become\",\"reversing\"],[\"reversing\",\"entrained\"],[\"entrained\",\"merchandise\"]])],[*,\"I could help more people become immortal\",a_alg([[[[[],n],v],\"more\"],[\"more\",\"layered\"]]),b_alg([[[[[],n],v],\"more\"],[\"more\",\"layered\"]],a),bb_alg([[\"more\",\"layered\"]])],[*,\"I could help more people become immortal\",a_alg([[[],\"could\"],[\"could\",\"creeper\"],[\"creeper\",\"conformations\"]]),b_alg([[[],\"could\"],[\"could\",\"creeper\"],[\"creeper\",\"conformations\"]],a),bb_alg([[\"could\",\"creeper\"],[\"creeper\",\"conformations\"]])],[*,\"I could help more people become immortal\",a_alg([[[[[],v],a],\"could\"]]),b_alg([[[[[],v],a],\"could\"]],a),bb_alg([])],[*,\"I could help more people become immortal\",a_alg([[[[[],v],a],\"help\"]]),b_alg([[[[[],v],a],\"help\"]],a),bb_alg([])],[*,\"I could help more people become immortal\",a_alg([[[[[],v],a],\"could\"]]),b_alg([[[[[],v],a],\"could\"]],a),bb_alg([])],[*,\"I could help more people become immortal\",a_alg([[[[[],v],a],\"people\"]]),b_alg([[[[[],v],a],\"people\"]],a),bb_alg([])],[*,\"I could help more people become immortal\",a_alg([[[[[],v],a],\"help\"]]),b_alg([[[[[],v],a],\"help\"]],a),bb_alg([])]]
[]
[1mtrue.[0m


noprotocol.


"]