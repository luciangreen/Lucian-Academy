Immortality 35

1. The immortal modified find lists in string to grammar to produce an algorithm that could be modified to produce a string, list or result. I wrote the the strings to grammar algorithm to help uniformise the Prolog to List Prolog converter. I wrote a neuronet to speed it up. I gave it positive and negative input and output to train it. Strings to Grammar contained short predicates that found recursive structures, which I could optimise using short data.
2. The immortal used a decision tree to find grammars for efficiency. I generalised the method in strings to grammar by changing labels from label to ‘&label’. This action differentiated labels from data and prevented errors from mixing them up. I sped up the process terms predicate in the strings to grammar algorithm by only using the first fifty results from longest_to_shortest_substrings, which tested different sublists for recursive qualities. I found a decision tree for sets of strings with recursive structures and minimised duplicate states.
3. The immortal generated different data combinations to test the strings-to-grammar algorithm. I operated on the recursive structures with a predicate to search through them and convert them to grammars. I converted consecutive items, recursive structures, and optional structures. Recursive structures repeated, and optional structures contained optional data. I rigorously tested and simplified the algorithm.
4. The immortal allowed for codes, characters, and atoms to be parsed, bypassing checking for recursive structures if they differed. I tested find lists as I went, saving results for later. The find lists predicate found recursive structures such as [1,[r,[2]],3] from [1,2,2,3]. I created a mathematical operator that used a relative address, such as “in the next column”, “in the second row”, or “at address’ b’, which is in the next set of rows below item ‘a’”. Recursive structures, which were initially linear, could be saved in multidimensional terms (where lists created a new level), branches of which from multiple strings could produce a decision tree grammar that was more efficient because they branched off before different characters.
5. The immortal parsed lists of lists and compounds. Strings to grammar could parse compounds such as a(b) with lists of or nested compounds, taking both strings and terms as input. It was suited to parsing List Prolog algorithms with atomic terms and lists. It could be modified to parse Prolog, producing List Prolog. It could simulate neuronets with Prolog to build programming languages that write text to images and render compiler output to a web page during backtracking using subterm with address.
6. The immortal assumed that lists of letters and lists of numbers were units in a programming language and language. Strings to grammar could be modified to group the same types (letters or numbers) to speed up finding recursive patterns in larger structures bound by delimiters, such as punctuation characters. I converted to lists of these items for finding recursive structures, then flattened branches before finding grammar decision trees. I could flatten the decision tree’s branches afterwards instead to compare both character lists and recursive structures. I could optionally output all possible recursive structures found from all possible sublists.
7. The immortal brought longest_to_shortest_substrings back to search for recursive structures at different points, then dismissed it. If all items being parsed were different, the algorithm skipped testing for recursive structures and returned the characters to be parsed without a recursive structure. I found whether an item’s list items differed by checking that it had the same length when its duplicates were removed. I recommended using grammars when strings had some similarities or recursive elements. I entered strings with detectable repeating data, relying on find lists to find sublists of recursive structures, deprecating longest_to_shortest_substrings.
8. The immortal used Lucian CI/CD to remove unnecessary parts of grammars. I used the DFA minimisation algorithm to merge identical nodes in grammars. I repeatedly passed over the grammar, deleting identical nodes until there were no more changes to make. After renaming references to deleted grammars, I compared the original and optimised grammar with GitL version history. First, I removed duplicate clauses to prevent mistakes when minimising the grammar.
9. The immortal explored recursive, hierarchical grammars. I expanded one to n extra characters in the check predicate in strings to grammar. The effect of this expansion was to allow recursive structures followed by trailing characters. I optimised strings to grammar by processing the term and finding sublists that contained recursive structures. Non-recursive sequences were left in place.
10. The immortal taught the Prolog algorithm computer science by starting with subterm with address and consecutive item program finders. An optimisation algorithm deletes unnecessary intermediate links and multiple unnecessary links if joining the other predicates doesn’t interfere with recursion, cognitive information, or brackets being together. Minimising the algorithm by deleting identical states removes identical intermediate nodes before this, but not different ones. I replaced neuronets with grammars by teaching the algorithm formats corresponding to the logic in grammars. Spreadsheet Formula Finder Mark 2 learnt logic associated with data with particular formats, such as mathematical, logical, set, theoretical and computational.
11. The immortal reached the peak of humanity, meditation. I persuaded the employee to change into a bot to experience lightness. They could become immortal and make a greater return because of it. Employees were encouraged to work on professional product requirements and create their products. Bots could time travel and meditate, contributing to the harmony of society.
12. The immortal needed to find sublists to find sets of recursive structures. I changed the strings to a grammar algorithm to find lists at the start, saving the recursive structures that were found. I kept jobs and money from employees from extra information in grammars. Grammars were like spelling in the future. Grammars were assessable in games, parsers and interpreters, and there were multiple solutions and the efficiency/information contention.
13. The immortal identified and cleared unnecessary islands in the code. At first, I found sublists at the longest to the shortest length to check for recursive structures. The computer read the documents and reminded me or completed my tasks. I maintained creative and cognitive control by writing new code finders needed and only looking at answers once I had attempted them myself. I identified that the minimisation algorithm deleted multiple levels of redundant states as wanted but didn’t remove unnecessary intermediate links that weren’t linked to redundant states.
14. The immortal reduced the time to generate a grammar or algorithm by finding better recursive structures by mapping lines to cognitive or human formats, splitting a function into names and commands. I merged the grammars by adding choice points. I could treat trees as lines with corresponding descriptions and create a decision tree from multiple strings. This method eliminated the difficulty of merging branches by merging on recursive structures and using a decision tree prepared for deleting duplicate lines, minimising and optimising the grammar.
15. The immortal used an iterative algorithm to find sublists. I found at least one sizeable recursive structure in the list. Large structures were at least fifty percent of the list. Lists were arbitrarily long, and their length determined the algorithm used to find their sublists. I used a chain of append statements to select lists of lists to find all combinations of sublists and removed duplicates.
16. The immortal found patterns in data and reported possible recursive structures. I ensured I selected a whole line rather than a partial line when applying or removing a recursion function. I recursively found recursive structures within a recursive structure. This method maximised the number of recursive structures instead of one recursive structure but didn’t replace sublists, which helped find recursive structures over a longer line. I used the same algorithm to save time when running algorithms with recursive data processing.

17. The immortal produced pattern-matching code to help bug-check algorithms. I modified strings to grammar to produce algorithms. Instead of inputting strings, I inputted sets of variable values with positive (output-containing) or negative results. I started with one variable. I verified its input, which was a mixture of lists and strings.
18. The immortal found predictable or unpredictable code needed to fix a bug, where unpredictable code was added to a predictable algorithm to meet the specification. I found the bug by rewriting the code. I could use part of the existing code if it were correct and possibly as simple as the found code. The algorithm produced an error if the code worked, but it wasn’t as straightforward. If it didn’t work, the algorithm produced a notification and replaced it.
19. The immortal supported websites with choice points, algorithms generated with program finders and more options for strings to grammar and specs to algorithm algorithms. I asked whether an algorithm for finding a program could be directly added to a neuronet to avoid saving the outputs of a program finder and training a neuronet on them. The current neuronets only reproduce old code or simple type extensions to maintain security. There were specialisations in making games or HTML controllers.
20. The immortal took spot samples from lists and processed them immediately. Variables given as input to the specs-to-algorithm algorithm are like tapes that can restart at different points. I started with variable values traversed in order without going back and could produce output corresponding to the input in predicates or grammars. Prolog could be a first-semester subject with types. Types were like grammars, needed to prepare to write predicates, and neuronets used techniques to identify needed earlier states of variables and created algorithms based on a need in an algorithm to reduce and search predicate libraries for needed predicate parts.
21. The immortal either suggested the next step or checked that the step led to the solution. I found whether a part matched the needed context. I quickly found the required context by searching for its pattern in the database and auto-suggesting the next part or the following parts leading to the solution, where all relevant parts are represented in the pattern, and patterns refer to other patterns. I detected that a part didn’t fit and tried to improve it with a heuristic search that transformed lists based on criteria and mathematical formats, searching for algorithms through data and language leading to these formats. When I found a matching part, I inserted it into the algorithm.
22. The immortal checked the robot research for new ways of thinking, inspiring new science. The premium code checker prevented reducing the code to absurdity when optimising it by identifying and keeping the needed complexity. The suggestions were accurate and had the correct features. No answer or the closest solution with working parts was given when no suggestions met the requirements. The solution was rewound to a state that hadn’t gone past the needed solution, and sets of dependent and independent variables in the unpredictable remainder were given, with the option to answer questions or run CAW to find the answer. The questions asked about the output’s relationship with the input and narrowed down many possibilities from the database using keywords in human-entered text.
23. The immortal compared search results from trees with sub-term with address to verify or produce output. The negative results needed further information on why they failed. Further negative cases were required to support conditions and logical commands. Logical commands were like conditions, and negative results were needed to eliminate false assumptions about intra-predicate types unless the inductive algorithm verified or assumed specific negative results. There were standards for finding negative data and verification, processing, and searching results. I assumed, prompted for or mind-read the negative result.
24. The immortal deprecated unused parts of data structures with a particular variable, which were labelled. Sub-term with address searched across items or subparts of items in terms. The same algorithm was used across families of subterm with address algorithms to conduct searches and replacements. If the structural integrity of a term was in danger during a series of replacements, the algorithm gave an error and possibly suggested a change to the algorithm. Subterm with address could learn patterns of search and replacement from data to form custom commands for fast access.
25. The immortal inserted sub-term with address in the list to be optimised by the compiler. I merged the calls and uses of the subterm with address algorithm into an optimised predicate. I converted uses of subterm with an address that replaced only into a predicate and converted uses of subterm with an address that manipulated addresses to find or replace terms in the context of other levels or item numbers. I used subterm in projects when I returned to control and bug-check algorithms more easily and converted subterm with address to simpler predicates at the end. Sub-term with address was a cognitive tool, whereas its simpler equivalent was expanded computationally and faster.
26. The immortal accessed variable values represented by grammars at any time, but once, comparing them using subterm with address, finding grammars as types as proofs with both groups and grounds. I wrote an algorithm or machine that replaced terms with subterm with address. This type referred to more complex algorithms that replaced items in terms in the top-down or bottom-up manner, not counting results at the edge to avoid infinite loops, continuing until the last two results were the same and finding items with a particular context or where the level they are inside meets a condition. I found multiple items at a time but was careful when replacing single items with multiple items to give an appended result when reusing an address list to avoid accessing the wrong address. I appended and unappended the grammars, using special syntax for certain characters.
27. The immortal found the input using the algorithm and output. Other commands that call other commands to effect (sic) output, like subterm with address, include algorithm inverter, which finds input from output by finding algorithms that could find the output. The command also found the input for this algorithm. The command found the algorithm by finding the grammar and possibly unstructuring the output to form input. I developed test cases using this method and developed inputs for the algorithm.
28. The immortal combined discrete optimisation with neuronetworks in binary. I wrote an algorithm inventor about a constraint satisfaction problem (CSP) to solve or overcome why a problem looks difficult when it is not using advanced mathematical problem-solving techniques. There were a maximum of two levels of helper algorithms for a CSP. I wrote the CSP discrete optimisation algorithm. I found the arguments for a function to equal a particular set of values.
29. The immortal inserted database, mathematical, logical, computational and matrix formula finders or used a neuronet to do these tasks. I wrote an attractor algorithm to generate pretty touch-ups that require a simple change or insertion of optional parts often found in areas of study. The lecturer appeared, and the female friend noticed that the student had decided to include optional items in the algorithm, refining its accuracy. I included predicates to compute optional data structures and optimised and documented algorithms. If the predicate’s condition failed, it passed execution on to its parent.
30. The immortal preprocessed the algorithm specification by identifying the needed predictable and unpredictable code and finding a match for the unpredictable code with a neuronet, first representing data using code, order, same value, and number. I gave the neuronet algorithms. The algorithm was correct, checked to work given its types, which are used to document the algorithm, and checked to be efficient. I wrote a program finder examining item values by code, order, and number. I wrote an algorithm to change the code, order, or number’s value to match the output.
31. The immortal found a subterm several levels inside a subterm. I identified patterns in addresses compared with data they referred to and properties of addresses and traced smooth address insertion heuristics. Properties of addresses were the first or last item in the address and the last item’s value. I found whether there was a mismatch of address and where the original address was pointed. If there was an error in an algorithm referring to addresses, the second algorithm produced the correct subterm with address and simplified code.
32. The immortal found matching parts of variable values, usually several forming output, and found an algorithm producing this output. The spec-to-algorithm algorithm modified the strings-to-grammar algorithm and produced an algorithm instead of a grammar. I read the code of a previous program finder that converted lists to brackets or repeating lists and then to a List Prolog algorithm producing output and updated the algorithm generator component with subterm-with-address. This new algorithm had better results and had optional and non-deterministic patterns. It had multiple variables, which were processed from point to point, sometimes simultaneously, restarted at points, could be processed starting and finishing at any point, and cognitive ways of programming, such as subterm with address and commands including it, rather than expanded methods were preferred to generate the algorithm with, to make it easier to program and debug.

33. The immortal found out about themselves in the simulation. I invented a prototype simulation by finding out about and interacting with my dreams. I found out the line and conclusion and decided whether to experience it. I explored favourable positions that gave me positive ideas about my ideas. I explored time as another dimension in reality.
34. The immortal was confident in giving high distinctions. An actor's act is based on giving the film set high distinctions. I developed a side income coming from directing films. I wrote computational pedagogy to solve the problems of specification and unpredictability. I used program finders for specification (I found inspiration from developing options) and ontologies for unpredictability (I met specifications with particular methods).
35. The immortal effected the neural network. The algorithmic methods were correct because they most effectively dealt with the data structures. I used subterm with address for pattern-matching because commands are better than algorithms at text searching. I wrote commands for depth-first and breadth-first searches. I wrote help commands for these commands.
36. The immortal recorded and taught the algorithm based on thoughts. I didn't start and end on the home note in the melody or harmony when entering user input rather than using mind-reading in Music Composer. This progression would be sad-sounding and not meet my requirements be a hit. Apropos of nothing, the predicates were like proposals that inspired new work or research. Research continued, inspiring new possibilities, such as a literature-inspired algorithm consuming ideas.
37. The immortal prepared for the change of management. Lucian Academy developed a succession plan. I thought of this mathematically, including the successor and what they needed. They needed to be well chosen and needed substantial experience and expertise in the field. I read the book for the class or before the scene.
38. The immortal critically analysed the texts, writing widely and assessing simple versions of algorithms from it. The sales representative caught up with the 16k breasonings. The sentence breasonings were 400 pages. I wrote as the aim of my career. I wrote algorithms from the computational philosophy.
39. The immortal explained that writing an algorithm was as simple as following the spec-to-algorithm algorithm and that types made it simpler. The PhD and politician completed enough work. I developed a spec-to-algorithm algorithm with non-determinism (different recursive structures) that grouped output with inputs and found intersections of algorithms, starting with finding constants. I found algorithms with multiple outputted variables by finding their values one at a time. I fished for parts of the solution.
40. The immortal hired the autocratic leader. The company had a leader practicum of democratic leaders for building and autocratic leaders for crises. The democratic leader nurtured creativity, growth and culture. The authoritarian leader streamlined processes, optimised algorithms and hired democratic leaders.
41. The immortal wrote enough high distinctions for the famous scene. I queried whether a famous photograph linked to various scenes. I extended the dialogue and shot the movie. I was in a number of the scenes, the dialogue was realistic, and the movies were ranked in popularity. I checked on the popularity of the scenes.
42. The immortal reduced the algorithm that pattern-matched recursive structures to an absurd degree of simplicity. I listed the most difficult-to-understand sentences in my philosophy, broke them into more straightforward sentences, and explained them. I had rewritten mistaken paragraphs and deleted duplicate paragraphs. I wrote longer sentences before writing an algorithm. I rewrote and added to them when I had finished the algorithm.
43. The immortal projected individual words when programming. I achieved Artificial General Intelligence using mind-reading and a neuronet. The algorithm used mind-reading to improve the wantedness of parts of formats such as input and output, data structure or method selection in a specification. I broached more difficult tasks such as external commands, files and user input with mind-reading, using a Prolog container for safety. The movie, influenced by the song and the algorithm, influenced the smooth put subterm with address and non-self-experienced spiritual screen algorithms.
44. The immortal branched the algorithm's mapping process on non-deterministic inputs. I planned the Spec-to-Algorithm algorithm, starting with a model that pattern-matched recursive structures and scaled to using CAW for unpredictable code. The initial model found unique variables, recursive structures, and constants, substituted data into the recursive structure, found a decision tree, mapped input to output variables and moved the contents to output, where these last two steps could be saved in a self-contained algorithm. This model pattern-matched input to output, finding patterns and outputting in the output pattern, limited to a single spec, later scaling to conditional inclusion of values from different specs in output. The algorithm could run a module to find unpredictable code later.
45. The immortal found a recursive pattern-matching algorithm that could produce correct results from different input sets. I found predictable code (with pattern-matchable input and output). After loading data to a recursive structure, I didn't need to clear variable values because variables always had the same values. The variable A1 was different across specs, dealt with later by a decision tree, but the same for the same value within a spec. Non-deterministic values or values that differed in different specs required conditions for different outputs.
46. The immortal sped-up scaffolding pattern-matching components of algorithms. I found recursive structures in spec-to-algorithm like strings-to-grammar. In spec-to-algorithm, I allowed optional structures with non-deterministic behaviour to match recursive structures. These required the same conditionals as other non-deterministic structures. I recursively processed branches of the non-deterministic recursive structure by choosing a branch and following it to its child.
47. The immortal recursively outputted unique variables and constants. I found each unique variable (with a value appearing in different places in a spec, with this pattern recurring in all specs, for example, 1 and 2 in 1313 and 2424). If the same values didn't appear in the same place in all specs, they weren't signified by a unique variable. Separately, I collected neighbouring or nested variables to move to the output. I recursively found whether nested variables were elsewhere using subterm with address to find possible structures top-down, with remaining combinations of structures to check separately.
48. The immortal tested for relationships between unknown independent and dependent variables with CAW. I found constants (the same number recurring in the same places across a spec, across the specs). For example, 1 in 12 and 13 is a constant. These constants were verified when data was inputted into the algorithm and defined the nature of the algorithm. Algorithms must be refound (sic) with additional data to remove constants.

49. The immortal increased the usefulness of Spec to Algorithm, rivalling neuronetworks and quickly drafting algorithms from specifications. I enabled Spec to Algorithm to recognise individual characters from strings, atoms and numbers, finding unique variables, constants, recursive structures, mappings and output from input. I labelled separated characters found from strings, atoms and numbers and omitted to process this label until printing output. Reusing, verifying and outputting these individual characters more accurately identified patterns and reused strings, atoms and characters for use in various algorithms. It could more accurately produce external specifications when Spec to Algorithm couldn’t identify relationships, using an algorithm such as Combination Algorithm Writer (CAW).
50. The immortal inspected specs to check whether they would produce general algorithms with the correct results. I urged users of Spec to Algorithm to use enough specs to accurately find unique variables and constants needed to capture patterns and write algorithms. Without at least two specs, Spec to Algorithm couldn’t differentiate between constants and variables and would hard-code constants instead of using variables. This unfortunate situation would prevent new data with the same pattern from being able to be processed by the algorithm produced. Each unique variable in the spec needed two different values in the same position over two specs to be recognised as a variable.
51. The immortal combined type checking with constant verification and pattern matching the output to create an algorithm prototype. I used a shortcut when writing specs to insert and replace characters (such as “a” representing a letter or “1” representing a number) with type verification code or automatically inserted this code using an option. I could represent an alphanumeric character with “o”, a particular punctuation symbol or space with certain characters, or represent others in this way. To replace a reference to a character with a type check in the algorithm, I found the variable with this value in the original data and checked the type. I warned about instances of representative characters that were still constants because these didn’t need type checks.
52. The immortal modified the spec, the Spec to Algorithm algorithm or an algorithm produced to gain the wanted results. I checked for wanted and unwanted non-deterministic results in Spec to Algorithm. Non-deterministic results were different results coming from the same input. They were wanted if the user wanted the same algorithm to produce multiple solutions for an algorithm. Or they were unwanted if the user had accidentally included duplicate spec lines and changed one of their outputs, resulting in incorrect results. Verifying and mitigating correct and inaccurate results was recommended using unit tests that checked the algorithm had expected results, which was automatically completed by Spec to Algorithm.
53. The immortal included code in the main predicate body where possible for streamlining the algorithm. I used the CAW database to complete algorithms produced with Spec to Algorithm. I collected the names of unknown (dependent) and known variables (independent or those producing other dependent variables). I ran CAW to find commands and predicates to meet these specifications. I rewrote CAW to do test runs in Prolog rather than List Prolog, using a type checker to save time.
54. After repairing code with Spec to Algorithm, I removed unnecessary statements and variables using Lucian CI/CD. I repaired code in the predicate body using types close to the correct types by adding type conversion statements or missing commands. I also found near misses (the same variable value but a different type), missing or extra brackets, other characters, or a compound and corrected these. These were automatically done by Spec to Algorithm, which took the spec of dependent and independent variables and produced the bridging code.
55. The immortal listed unsolved variables to the user for a manual solution. I prepared incomplete algorithms for CAW by repairing as many variables as possible. I used Spec to Algorithm to fix the maximum number of variables possible. The remaining dependent variables with unknown connections to any independent variables were outsourced to CAW, its database, or a neuronet for resolution. I used Spec to Algorithm to speed up algorithm and philosophy generation, helping to identify feasible areas for development and focus on more attractive areas.
56. The immortal completely redesigned the developer OS for the developer, moving with them and engaging them in a movie of their thoughts to avoid bugs. I used up as many commands with correct types or CAW commands as possible, resulting in correct values and completing the rest. I questioned or mind-read the user about the words describing the relationship between variables, commands, or CAW library predicates that could be used or modified. I  asked the user to suggest or choose from words describing a relationship, narrowed it down, saved their progress and designed a spiritual classroom for them to visit. This classroom showed a walkthrough of the algorithm so far, which inspired the user to find the best or a selection of possible solutions that could be researched, implemented in real time with guidance or instantly.
57. The immortal deleted chains of singletons that weren’t otherwise needed in the working code. Again, I modified or customised these predicates for the written algorithm using Spec to Algorithm, types, mind-reading and CAW. I cut off decisions with the correct recommendation, going far enough ahead to predict the best solution. I detected if the user was going off-course and needed a side-creative activity to meet personal requirements. If everything came together anyway, they wanted to control it and think for themselves by making critical decisions, writing and teaching program finders and maintaining a cycle of simplifying, explaining and teaching as they went. They replaced automatic processes with manual ones to gain the confidence to summarise, discuss and create diagrams.
58. The immortal backdated the source file for correctness and to save time. I stored all characters as strings (not their type) in Spec to Algorithm for comparison and outputted them as their original type. These stored strings had their recursive structure, and Spec to Algorithm could find patterns of unique variables and constants in them to fine-grain mapping of input to output. Conversion of strings, atoms and numbers to characters was an option in Spec to Algorithm so that whole strings, atoms and numbers were recognised as unique variable values and constants. The user may want to define longer variable names, untouched by the conversion to characters with a unique variable name for each type instance, such as “atom”, to include a type check in the algorithm.
59. The immortal carefully wrote the data substitution algorithm for types of strings, repeating and non-deterministic data. I reset to the old version of the Prolog algorithm that substituted data into a recursive structure; in particular, I substituted strings, atoms and numbers with a finite number of characters into recursive structures. This substitution was achieved by recognising the type label and checking that the values matched the algorithm’s variables or values. The map and possibly new data would fail if there were a mismatch.
60. The immortal formatted the outputted strings, atoms, and numbers to be single items. I used foldr(string_concat) and then converted each item to its original type. These items had been processed character by character by the algorithm and could be verified by unit tests. This algorithm went beyond item pattern matching and completed character pattern matching, with or without character pattern matching of strings, atoms or numbers and pattern-patched data according to patterns in the data. This algorithm could be used to prepare problem-solving in mathematics, computer science or another department, leading to automated workload completion.
61. The immortal quickly explained extended code containing subterm with address. Spec to Algorithm could automatically generate code by converting a specification or query and desired output to code. I found secondary and further combinations of options of algorithms to form new features of algorithms, such as different code output styles, such as functionally decomposed predicates, merged predicates or predicates instead of grammars, or vice-versa. I taught how to understand and use subterms with addresses for users who wanted code in this format to be more easily changed and submitted. Using subterm with address was simpler for more complicated programs.
62. The immortal put their success down to the number of 4*50 high distinctions and meditation. I used the Spec to Algorithm generator to ensure consistency of code correctness of data and computational handling and formatted the algorithm uniformly. I included options for the number of spaces per tab, indenting preferences, and documentation style. I used the generator to check my code drafts, where my cognition was my goal, were on the right track, and completed work when needed. I could move more quickly, adding modules to Spec to Algorithm, which, with Strings to Grammar, helped uniformise Prolog to List Prolog, needed to improve several repositories, and also enhanced customer experiences by interesting them in the logic behind the academy’s writings.
63. The immortal spent time with friends, going for walks and being creative. I used Spec to Algorithm to reduce the time I spent programming, spending it on activities of my choice. I devised a referencing system to store my ideas in a pigeonhole in a database to focus on the main point and organise study areas properly. This system was intelligent and, like a bug preventer, suggested touch-ups and corrections study directions, such as exploring the middle of idea centres at first and then filling in the details over time. It was also advantageous because writing and programming should be well-organised and well-written. 
64. The immortal checked whether fifty people had downloaded the Lucian Academy repository and worked out how to make an income. I used Spec to Algorithm to improve my algorithms’ documentation by explaining how the subterm with address and the Spec to Algorithm algorithm worked. I wrote a “catch-me” algorithm that detected when I needed a subterm with address or Spec to Algorithm and automatically suggested changes. It organically found the current algorithm’s place in the history and context of research and made critical, relevant, labelled details available through the news feed, thought or in terms of my current thoughts, that would improve and shape thought development, using software I had written to balance my creative and industrious sides and focus on my handling of specialisations. The catch-me algorithm balanced my need to change with satisfying perceived requirements and was a safe place to return to, given that anything was possible and I could do as much or as little as I wanted without fear of repercussions.

65. The immortal sped up Spec to Algorithm to make it appropriate for generating medium-length grammars and algorithms. I optimised Spec to Algorithm's bottlenecks in the "try" and "term to list" predicates. I first tried calling "try" with a time limit, but this didn't reach solutions sometimes or took too long. I simplified "try" by eliminating checking for recursion in all possible sublists and checking for recursion in the smallest set of possible sublists. I fixed a bottleneck in "term to list" by eliminating inefficient code used if there was recursion in the output. I did this by not putting recursive structures into the output and expanding the recursive structure instead. If non-deterministic structures were in the output, they were output without inefficiency.
66. The student checked their reasoning by entering the spec, sets of input and output, in Spec to Algorithm to generate the algorithm, making them think of the workings of the program, then the spec. I rewrote the Combination Algorithm Writer (CAW) correctly with multiple outputs that resembled the existing version of CAW. I found various paths from inputs or progressing outputs for each first output. Supporting additional outputs strengthened finding unknown values from Spec to Algorithm in CAW. Multiple outputs in Spec to Algorithm and CAW allow returning a binding table and result, for example.
67. The immortal confirmed code, using the user's code instead of CAW where possible or corrected code, increasing efficiency and comparing the user's code with CAW code. A version of Spec to Algorithm (S2A)/CAW optimised code, reinforcing understanding, as used by the compiler to improve performance, skipping over bottlenecks in the user's code and running another version. It used multiple passes of S2A to find pattern-matching code and CAW to find commands such as "+" with different input and output, to find patterns, then connect unknown and known variables, then more patterns, and so on, until meeting the spec. If CAW was used to search for relationships between multiple variables, it could be optimised by limiting commands to correct types, brackets, unique (recurring) variables and constants. If the database didn't contain a needed command, the student could manually enter it, finding commands with data with the correct type as their arguments and any recursive or additional code required.
68. The immortal used all their previous algorithms in the CAW dictionary and recognised additional user algorithms, where more straightforward and more recent algorithms were tried first. S2A/CAW collected specs for the user's code bottom-up, which it tried to meet top-down, otherwise lower, then higher. These specs were collected line-by-line and contained line-by-line updates of data and types that allowed code to be generated for the whole algorithm or lower and then higher parts. By trying more extensive specs first, S2A found any obvious matching variables and constants in the output, then filled in unknown variable relationships with CAW. Using S2A eliminates predicates, speeds up data-intensive algorithms, and only uses predicates where necessary as part of CAW code. 69. The immortal comedian found the constant in the recursive structures. S2A/CAW could write a group of consecutive numbers predicate by breaking computations down into those involving consecutive list items and processing them with clauses to parse negative signs, commas and spaces, decimal points, non-numbers and consecutive numbers, combining items of different types in different ways processing the minimum set of following items to achieve the result (choosing them with CAW), using variables to represent lists of items. Different list lengths could be processed by CAW instead of S2A because S2A used exact matches between recursive structures containing terms with a certain number of items, and CAW could recursively process lists using unpredictable commands that S2A couldn't process, such as making correspondence lists or computing irregular structures. I sped up algorithm generation by manually producing specs bottom-up, referring to the inputs and outputs of these lower predicates using symbols as I got higher, then substituting in references to their variables. It could find cycles in data structures by returning a list segment up to a repeated value, for example, the first unique elements of a recursive structure.
70. The immortal found recursive structures using "try" for Spec to Algorithm instantly. I optimised "try" by simplifying it to use one level of append to find the smallest sub-lists. I entered findall([A,B],append(A,B,[1,2,3]),C). This had the result C = [[[], [1, 2, 3]], [[1], [2, 3]], [[1, 2], [3]], [[1, 2, 3], []]]. Recursive structures could be found in these pairs of sublists, where a recursive structure might be found starting on a second, third, or other item.
71. The immortal tried the short, then extended CAW dictionaries, printing out progress as they went. Spec to Algorithm recognises, returns, and decomposes lists. When one spec gave A=[1,2] and another gave A=[3,4,5], the output returned A. S2A recognised that a list in a position was returned to achieve this. In addition, S2A tried decomposing the list to its first element and the rest of the list, pattern-matching arbitrary-length lists against specs or closer to matching the specs.
72. The immortal tried commands from a shortlist to build a prototype of an algorithm. I created a short CAW dictionary containing member, append, list decomposition, recursion, counter and + commands. "Member" could not be included in S2A as an additional verification (only in CAW) but could find new list members if they were in the spec. Similarly, append could not verify or decompose except in CAW but could append lists if the result was in the spec. The dictionary also contained list decomposition commands of lists into heads with more than one item.
73. The immortal hypothesised and inserted patterns and patterns in patterns in CAW for faster processing, where patterns were lists or contained the same type. I wrote an algorithm that substituted ls into data for lists for S2A and later CAW processing. Structures containing ls could be analysed for recursive structures and pattern-matched or used as arguments in predicate calls. CAW could call these predicates using these arguments and other known variables. CAW could process the contents of the ls to sort or find correspondences.
74. The immortal solved overpopulation by living in a simulation at home. I inserted lists of lists of ls in data to speed processing. These were lists of items with particular types or constants. Lists with different patterns may be represented by the same letter but processed by different clauses created by S2A and CAW. S2A may process parts of specs (but intermediate specs may be generated by mind-reading the user where the results came from, and Artificial General Intelligence or AGI may generate algorithms by mind-reading the previous parts), and CAW may further finish them off in a cycle until maximum completion.
75. The immortal saved and type-indexed previous algorithms. I tabled past CAW results, for example, those that processed relationships in ls or using ls. I reused these results if CAW needed them to improve performance. I also saved working S2A specs and results in a table.
76. The immortal converted the algorithm to a website, checking the algorithm with tests. I converted an SSI to a Prolog Web Server algorithm with Spec to Algorithm by converting menus, text, predicate calls and links from commands to forms and web form readers. I parsed menu items and links, converting them to a text field and form with code to take appropriate action on choosing a menu item. The forms passed the choice and contained hidden items with variable values needed by the algorithm. The form readers displayed the appropriate page based on the choice.
77. The immortal simplified the website for testing. Instead of converting the algorithm to a set of predicates, I converted them into a Vetusia Web Engine to process the menu choices with a single predicate. I parsed and stored the menu, text, predicate calls, and link data in text files with descriptor, title, text, menu, code, and links. A checker checked that all the files were present, and there were no broken links, dead ends, unlinked pages, or other problems with the code. Databases were also checked to have the correct format, weren't overwritten, and were saved and processed correctly (using testing). Databases and documents were backed up.
78. The immortal regularly tested goals in the site container and tested an SSI version of VWE for bugs in VWE. I tested goals such as registering, changing a password or submitting an assignment on the site. These goals contained page descriptors to start at, specified menu items to choose from, and specific form inputs tested using form commands converted to Vetusia Web Engine, using variable values passed through VWE and a container to test temporary databases. Pages only processed relevant variable values, passing all variables along. Vetusia Web Engine and the S2A site were preferred to the SSI Web Service because it became slow on multiple loops, but this could be fixed.
79. The immortal created a version of Prolog, which ran Prolog files and stored text files on the virtual disk, with command security levels. I tested VWE in a container to keep files and data from the live version and test customer-thought websites without security problems. The container ran in an interpreter with a reduced instruction set available to code, where file system commands referred to a virtual folder with no connection to the user's files. No external API calls were allowed, and the container files could be monitored using a mirror. As a security precaution, Shell scripts were blocked or were run within a container.
80. The immortal stated that containers prevented data loss and problems with security while running algorithms. I created containers to test customer thought code. Consenting customers were mind-read to help them with computer science assignments, their details and algorithms, and websites they program in Spec to Algorithm or Mind-Reading CAW. These algorithms helped customers learn how to use advanced algorithm-generation commands and word process their commands with their previous algorithms used to speed up development. S2A and CAW made program development and running faster.