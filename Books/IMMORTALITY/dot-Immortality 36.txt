Immortality 36

1. The immortal modified the Spec to Algorithm (S2A) specs to return unknown variable values. In S2A, I checked for stored recursive structures for a particular pattern. I checked for these patterns in previous instances of running the algorithm. I made CAW-like recursion with dissimilar list items faster by giving formulas for transformations, appends using (:), and string concats using (&). I gave the Prolog algorithm as an example.
2. The immortal gave the spec for the append predicate, which could be modified to reverse.
% a([1,2,3],[],A).
% A = [1, 2, 3].
a([],A,A).
a(A,B,C) :-
 A=[D|E],
 append(B,[D],F),
 a(E,F,C).
3. The immortal rewrote append in one or two lines, as the compiler would to optimise it. The append predicate is represented in S2A as the following spec.
Inputs: A=[],B=a, Outputs: C=a
Inputs: A=[],B=b, Outputs: C=b to achieve general variables.
Inputs: A=[length,[d,1]]:e,B=b, Outputs: C=[a,[e,b:d]]
and another spec to achieve general variables.
4. The immortal simplified the spec for easier entry. The S2A spec could be represented as a spec with code similar to Prolog, as shown below.
Inputs: A={d}:e,B=b, Outputs: C=a(e,b:d)
The computation is represented in the following way.
Input=[1,2,3]
A={1}:[2,3],B=[],C=a([2,3],[]:{1})
Input=[2,3]
A={2}:[3],B=[]:{1}=[1],C=a([3],[1]:{2})
Input=[3]
A={3}:[],B=[1]:{2}=[1,2],C=a([],[1,2]:{3})
Input=[]
A=[],B=[1,2]:{3}=[1,2,3],C=[1,2,3]
5. The immortal saved time by printing multiple outputs as one or separated them if this was more helpful. If there had been more than one output of "a" (as a single S2A output may have multiple items), it could be represented as a(...)^1, where 1 is the item number of the output. Alternatively, this could be defined as two outputs. This representation involved changes in generating and running the algorithm, including whether outputs are single items determining whether they have brackets. Multiple outputs from a predicate save time and effort when programming by reusing computations.
6. The immortal simplified append to one input and output in Prolog. Append could also be represented in the following way with two arguments instead of three.
% b([1,2,3], A).
% A = [1, 2, 3].
b([],[]).
b([D|E],[D|B]) :-
 b(E,B).
7. The immortal optimised append. The form of append with two arguments is represented in the following manner.
Inputs: A=[], Outputs: B=[]
Inputs: A={d}:e, Outputs: B={d}:b(e)
and another spec to achieve general variables.
8. The immortal used fewer computations for append//2. The computation for append//2 is represented in the following way.
Input=[1,2,3]
A={1}:[2,3]=[1,2,3],B={1}:b([2,3])
Input=[2,3]
A={2}:[3]=[2,3],B={2}:b([3])
Input=[3]
A={3}:[]=[3],B={3}:b([])
Input=[]
A=[],B=[]
A=[3],B=[3]
A=[2,3],B=[2,3]
A=[1,2,3],B=[1,2,3]
9. The immortal gave the spec for the append+1 predicate, demonstrating a mixture of append and (+).
% c([1,2,3],[],A).
% A = [2, 3, 4].
c([],A,A).
c(A,B,C) :-
 A=[D|E],
 F is D+1,
 append(B,[F],G),
 c(E,G,C).
10. Append+1//3 could be modified to append+1//2 including:
Inputs: A={d}:e, Outputs: B={d+1}:b(e)
The form of append+1 with three arguments is represented in the following manner.
Inputs: A=[],B=a, Outputs: C=a
Inputs: A={d}:e,B=b, Outputs: C=a(e,b:(d+1))
In addition, another spec was used to achieve general variables.
11. The immortal optimised out unneeded clauses of deterministic predicates, automatically working out the type of clause Mercury Language influenced. The computation for append+1//3 is represented in the following way.
Input=[1,2,3]
A={1}:[2,3],B=[],C=a([2,3],[]:{1+1})
Input=[2,3]
A={2}:[3],B=[]:{1+1}=[2],C=a([3],[2]:{2+1})
Input=[3]
A={3}:[],B=[2]:{2+1}=[2,3],C=a([],[2,3]:{3+1})
Input=[]
A=[],B=[2,3]:{3+1}=[2,3,4],C=[2,3,4]
12. The immortal outputted data without performing calculations using S2A by default, and they could be computed later. The append+1//3 predicate could be modified to use lazy evaluation or only evaluate computations when needed, for instance, with a comparison or at the end, which necessitated the computation. Lazy evaluation leaves the evaluation of a computation until the last minute. For example, it computed 1+(1+(1))=3 or {a}:{a}:{a}=[a, a, a] just before comparison checks this variable or is output at the end. Comparisons are (=), part of verifications of if-then conditions.
13. The immortal found outputs in terms of two or more functions. An output may be a function of other function(s). For example, a(b)+a(c), string_concat(a(b),".") or string_length(number_string(a(a)+1)). In the case of a(b)+a(c), the results of these subsidiary functions are found and added. If the expression is 1+1, it may only be computed (as part of a lazy evaluation) once needed.
14. The immortal stored statements needed by outputs in the order they were needed. In addition, an output may use intermediate code that reuses the results of functions. For example, c=a(b), Output1=c, Output2=c. The first statement is stored with the function to find Output1, written as Output1=(store(c=a(b)),c). If more than one statement needs to be stored where Output1 contains more than one variable, it is written store((s1,s2)).
15. Unless directed otherwise, The immortal tabled and skipped over computing the algorithm. If an S2A spec gave the input [1,2,3] and output [1,2,3] where no other specs suggested foldr(append) was needed or list items were reversed, the algorithm was optimised to remove the append statement. If another spec suggested append was required to fulfil the function of foldr(append), append was kept. One optimisation may lead to a chain of optimisations that the first optimisation led to. If a stored or other function is always 0, 1, [] or ", this may lead to optimisation and ensuing optimisations.
16. The immortal wrote CAW with types with a neuronet trained on CAW's data. First, I wrote CAW with types, then trained the neuronet on its data. The neuronet found a certain number of possible commands matching the types. Then, the algorithm found a certain number (a small number, to keep complexity down) of commands using depth-first search. Then, it tested the finished algorithms with the Prolog interpreter, stopping when reaching a solution. It ran Prolog using higher-order programming, for example, A=true, A, where A was queried.

17. The immortal simplified the clauses to include the mapping, i.e. change 'A1' to A. I corrected Spec to Algorithm's mappings to correspond to clauses from the input. S2A produced clauses from the input. The correct mappings, from input to output, found from the input's recursive structures corresponded to these clauses, found from the initial input's recursive structures, constants and variables. Using an address mapping system was superior to unification for complex structures.
18. The immortal returned multiple solutions to a problem. S2A produces non-deterministic (multiple) outputs to generate various solutions to philosophical problems. The generated algorithm gives different outputs with the same or partially identical input. This process requires counting the unique groups of inputs, splitting non-one-length groups, and joining one-length groups to find constants to keep constants in groups with the same inputs. However, duplicate specs are removed, so it is only possible to give a duplicate answer by providing a duplicate generated algorithm.
19. The immortal matched, reused, and matched groups and wrote code containing variables referring to data structures. The code generator found and returned groups rather than individual variables. I used an algorithm that found whether a list, a sublist or a substring was returned. These substrings could be taken from strings, atoms, numbers or non-list compounds. The algorithm found whether these groups recurred in the input and output, created variables and generated code to process them.
20. The immortal skipped and retried parts of the nested formula in Prolog when debugging. I converted from Prolog A=a, B=b, f(A, B, C) to the faster formula C=f(a, b). These formulas had single or multiple outputs and were faster because inputs and outputs were defined. Functions could be nested, and S2A specs could quickly be developed and converted to Prolog. If necessary, I kept them in this format for speed, requiring a different interpreter.
21. The immortal sped up finding recursive structures with a neuronet when optimising the algorithm. I verified the recursive structure and map of the algorithm generated in S2A. This step increased the accuracy of the verification algorithm and allowed faster development. The pattern-matching algorithm called a predicate with these as arguments, shortening the algorithm. I used S2A to help compile the algorithm, first optimising it bottom-up.
22. The immortal replaced move vars with a foldr call in S2A. I did this by changing the following call:
move_vars1(T1, Map, T2_old, Out2)
query:
move_vars1([1, 2], [[[1, 1], [[1, 2]]], [[1, 2], [[1, 1]]]], ['C2', 'C1'], Out2)
algorithm:
move_vars1(_, [], T2_old, T2_old) :-!.
move_vars1(T1, [Map|Maps], T2_old, Out2) :-
    move_vars_pred(T1, Map, T2_old, T2_new),
    move_vars1(T1, Maps, T2_new, Out2).
and result:
Out2 = [2, 1].
To query:
foldr(move_vars_pred([1, 2], [[[1, 1], [[1, 2]]], [[1, 2], [[1, 1]]]], ['C2', 'C1'], Out2)
with call and algorithm:
foldr(move_vars_pred(T1), Map, T2_old, Out2).
and result:
Out2 = [2, 1].
This optimised move vars. I replaced similar commands with foldr calls.
23. The Vocational Education assignment for State Saving Interpreter (SSI) evaluates the following algorithm. Put into practice SSI to exhaust choice points in "findall" using a "for" loop. Ensure the interpreter saves the state before each choice point. Use a "for" loop to iterate through all potential solutions. Restore the state after each iteration to evaluate the next choice point. Collect and return all solutions after the loop is completed.
24. The Vocational Education assignment for Time Machine assesses the following algorithm. Breason out 16k breasonings for time travel and 16k for immortality. Utilise BAG to find new word pairs AD from neighbouring pairs AB, CB, and CD. Implement S2A to find these pairs in sentences. Make new sentences with lateral and exciting significance. Exhaust all possible bonds in the given text.
25. The Vocational Education assignment for Lucian CI/CD evaluates the following algorithm. Identify dependencies in algorithms using CI/CD practices. Extract combinations of lines in sets of predicates from dependencies. Test the simplest combinations for functionality. Implement continuous integration and deployment for automated testing. Ensure the system reports on the effectiveness of each test.
26. The Vocational Education assignment for GitL evaluates the following algorithm: Store repository versions in a list of folders. Implement a system to post differences ("diff") between versions. Ensure the system can recognise the most recent version. Compare changes with the most recent version and generate a diff report. Display the results clearly and concisely.
27. The Vocational Education assignment for Spec to Algorithm (S2A) Algorithm evaluates the following algorithm. Use "try" from Strings to Grammar (S2G) to identify recursive structures. Locate constants that recur in all the same positions within specs of similar shape. Devise a decision tree to merge shapes using these constants—substitute information into the identified recursive structures. Map input to output using a pre-determined map and render the output from the filled-in recursive structure.
28. The following is the Vocational Education History and Philosophy of Science assignment for State Saving Interpreter (SSI). Discuss the historical development of state-saving mechanisms in coding languages. Explore the philosophical ramifications of state saving in computational logic. Analyse the evolution of choice points handling in logic coding. Examine the role of interpreters in the history of computer science. Present a case study on modern technology's practical apps of state-saving interpreters.
29. The Vocational Education History and Philosophy of Science assignment for Time Machine follows. Investigate the historical development and philosophical ramifications of time travel ideas in science fiction and abstract physics. Analyse the role of breasoning and its parallels in historical, scientific methods and theories. Explore the history of verbatim examples and their evolution in computational linguistics. Examine the impact of algorithms like BAG and S2A on contemporary verbatim processing. Present a case study on applying verbatim models in creating new and innovative texts.
30. The following is the Vocational Education History and Philosophy of Science assignment for Lucian CI/CD. Examine the history and evolution of continuous integration and constant deployment (CI/CD) in software development. Discuss the philosophical underpinnings of automated testing and its meanings in the history of computing. Analyse the development of reliance management in programming. Explore the impact of CI/CD practices on the efficiency and reliability of software development. Present a case study on the Strategy and benefits of CI/CD in a modern software project.
31. The following is the Vocational Education History and Philosophy of Science assignment for GitL. Investigate the history and development of version control systems in software engineering. Explore the philosophical implications of tracking changes and maintaining historical records in software projects. Analyse the evolution of "diff" algorithms and their meanings in computing history. Examine the impact of version control systems on collaborative software development. Present a case investigation on using GitL or a similar version control system in a larger-scale software project.
32. The Vocational Education History and Philosophy of Science assignment for Spec to Algorithm (S2A) is the following. Investigate the historical development of recursive structures in computer science. Analyse the philosophical implications of using constants and decision trees in algorithm design. Explore the evolution of mapping methods in computing and their historical meanings. Examine the role of recursive structures and mapping in the history of programming languages. Present a case study on the practical apps of the Spec to Algorithm method in modern software development.

33. The immortal first split and found recursive structures from, joined and found further recursive structures from sequences. I split lists on the characters "[]()" and strings on the characters " ,;.()[]" in S2A to initially identify and group recursive structures. Splitting these lists and strings to ten or fewer characters reduced the time needed to find recursive structures. I recorded the numbers of adjacent split character sequences and joined them so they were not separate lists or strings. These delimiters were split and kept separate to preserve existing recursive structures that could be joined after finding them.
34. The immortal computed the decision tree for the inputs. I grouped the recursive structures [r,[r,a]]=[r,a], [r,a] =[r,a] + [r,a] and a + b = c. I labelled list sequences split with split and string sequences split with split1. I simulated a neuronet in as few as no steps to find solution(s) to a problem. Working out the "neuronet" was more of it. 
35. The immortal queried whether it needed to take long to train, run, and store, as well as the analysis or how it could work out the data. For example, the shortcut to CAW connected certain combinations of prioritised keywords to solutions. "Natural language" was paraphrasing. Findall was lowering the focus when choosing solutions. Mind-reading was used as a popular distraction and to move our attention away.
36. The immortal enjoyed being inside the robot/simulation's neuronet and noticed the small changes. Neuronets had more decision trees as context. This data arose from the data around the input. I questioned whether this would slow it down or was necessary as it would likely have found the solution. Given the solutions as training data, the neuronets could return them by themselves, where multiple questions or answers might lead to worse solutions.
37. The immortal gave the text file solutions and an algorithm to give the mapping from a possible new input to a solution, similar to Spec to Algorithm. The robot preferred language to mathematics like a human but was faster. Harmony was the robot's driver, as pedagogy was a human's driver. The robot desired to be close to the human. Given more data and further validation, long algorithms or proofs were possible.
38. The immortal trained the neuronet's output using previous neuronets, with each step described by natural language. The neuronet recognised a recursive structure and produced an output or possibly a sequence of these computations. The input and output of the algorithms may have formulas pointing to other algorithms completed using the fastest shortcut algorithms available. If it didn't know the answer, it quickly said so. Accreditation thought of the noumenon clearly, with debugging being the most challenging skill.
39. The immortal could skip over irrelevant nested lists. The neuronet chose formulas in input with more extended decision trees, checking for comparable decision trees to know when to cut them off. It achieved this using a breadth-first algorithm similar to a perceptron learning one element at a time. S2A performed some of the neuronet's preprocessing, optimising or skipping over long lists or strings that the decision tree made obsolete. There was evidence that the neuronet returned program-like structures of its design when attempting to answer a question, which it had planned to expand into Prolog.
40. The immortal could help people to meditate when enough medical science had been discovered. The neuronet was limited in working out recursion in time, taking large amounts of data, including the possible input, and working out the answer. It could quickly compute answers seemingly intuitively using mathematics, but by analysing this logic, we can better understand and help improve its processes. I looked forward to the non-invasive sound surgery. How could it be researched non-invasively by simulations of tissue frequencies using dead bodies and working out frequencies from graphics analysis? Despite not seeming so, changing organism behaviour to cure diseases was possible during our time using meditation.
41. The immortal reduced multivariate to univariate relationships using dimensionality reduction (Principal Component Analysis (PCA), which I later didn't use) and decision trees. Optimising neuronets by cutting off decision trees using breadth-first search instead of regression is more intuitive and can be repeated multiple times. I skipped over the rest of the decision tree when I had found the answer. Neuronets were problematic. They might not pass or fail because they needed to process non-monotonicities (exceptions) in their rules methodically. The brain knows answers instantly, showing that neuronets' training could be optimised.
42. The immortal "got" the relationship with neuronets, as subterm with address got the desired result with a single command. I considered using PCA to find new features, which helped reduce dimensionality, capture the most crucial variance in the data, and improve the decision tree model's performance and interpretability. The eigenvector with the largest eigenvalue is the first principal component, the eigenvector with the second largest is the second principal component, and so on (https://www.billconnelly.net/). PCA identified values on the main components of the data, where users should not use PCA because decision trees operate with multivariate data. They find the decision tree from left to right through the variables.
43. The immortal offered a working module version of the chatbot. I remained resolute by exploring the meditation of decision trees with PCA and S2A as a neuronet. Principal Component Analysis, while used to reduce an extensive data set to a smaller one with the primary information in the set, was not needed because decision trees found relationships between variables. Instead of PCA, I wrote a manual neuronet that contained Spec to Algorithm. Spec to Algorithm contained decision trees and processed recursive structures with pattern matching, later processing different predicates.
44. The immortal controlled aspects of the neuronet, including their accuracy and speed. S2A found input and output formulas in specs with multivariate regression for each spec, a manual neuronet. I compared the accuracy of the manual and regular neuronet, where the manual neuronet was more accurate. I optimised the unused parts of trees. In addition, I introduced concurrency in S2A.
45. The immortal joined decision trees together for speed, where decision trees were considered as faster as mentioned because they had no training time and promised higher accuracy at the cost of needing more data. Using decision trees instead of regression was the contention for different specs rather than one large spec, which was necessary before. I drafted plans for a "neuronet" defeater that used decision trees instead of regression for more incredible speed and accuracy than neuronets. If a module didn't work, I researched and replicated its algorithm. After finding correlated variables in decision tree values in a variable (one dimension), I found the rest of the correlated variables in the maze of data. I connected to different parts of the labyrinth through another dimension.
46. The immortal used non-regression to examine neuronets. Non-regression performed the job of neuronets to optimise decision trees with multiple options coming from the same character using tighter decision trees with (numerically) prioritised (relevant or higher frequency) characters or abstract symbols representing the concept of a character. I removed similar (less relevant or redundant) branches of the decision tree by setting a threshold for the significance of each option, eliminating those that fall below this threshold. Duplicate options are merged. I embedded algorithms in decision trees to select or mind-read (to relax the brain, especially when all options are correct) more relevant and eliminate less likely options. These algorithms captured the essence of a sentence, whether they were a favourite example, algorithm or mind-readable idea to spurn interest and document the idea, giving enough detail to understand it.
47. The immortal optimised the non-regression neuronet. There are short and long versions of texts: one for computation, one for converting from input and one for output. Neuronet answers organised answers around critical ideas to output (with a seen-as version) based on heuristics about symbols in the text, with three levels of relevance and ten inferences. The algorithm is deeply interested in English as its seen-as version and thinks in this context as it analyses how characters or symbols appear to determine their significance. Often, the algorithm summarises its answers more generally for interest and understanding, linking to general themes for clarity and developedness.
48. The immortal included extra ingredients to help the neuronet function properly. Rule-based systems derived from knowledge are refined by testing and prioritising options when helping the system write answers. A different algorithm learns from the past and writes relevantly to the question involving non-regression supervised learning techniques, training the model with labelled data to optimise the tree from the performance metrics. Statistical correlations, not automatically done by non-regression, help understand the relationship between options and characters. Completing this separately from decision trees allows independent and human verification of robot thoughts.

49. The immortal specified the characters and symbols of the algorithms with the non-invasive algorithm or a headset. I mind-read myself writing algorithms. Mind-reading, a technique of an advanced society, helps reduce manual input and automate processes noninvasively and more simply. By thinking instead of developing overly complex systems, workflows become more straightforward to customise. Variables can represent function calls to write and edit specs quickly. 
50. The immortal claimed that the new mind reader version was more accurate and ethical. A mind editor may be activated to select, edit, or delete an element in one second. The algorithm selected the variable name. Then, it allowed me to choose whether to edit, move, or delete it. This action could also be accomplished using the word processor. 
51. The immortal gave the following line of code with the correct variable names, considering previous code and writing needed predicates. I planned to build users' algorithm dictionaries. When a student entered an algorithm in the editor, it was saved in the dictionary buffer, optimised, split into two-line predicates, duplicates removed, and added to the dictionary. After retrieving code from the dictionary, the predicates were joined. Using their old code made students learn faster and more comfortable developing their autocompletion features.
52. The immortal used their database of known code and debugging knowledge to debug code with a neuronet. I built models of students' work to help them. I mind-read each line of code back in time or read their repositories. The object reader prevented tiredness when reading screenshots. I rendered the screenshot from an angle and debugged it.
53. The immortal said that tricky debugging required mid-predicate specs or spec inspection to determine whether it was correct. When I had the current code, I mind-read the desired new features and their possible specs and applied them to the code, sometimes with hand changes. These features included changes to current specs and formulas in specs, possibly requiring new predicates and optimisations. Needed changes to code using the new specs were found by diffing the old and new code. I debugged the code and corrected it, given the specs were correct, and unwanted flow-on effects from new specs in the algorithm required deciding which specs to use.
54. The immortal read about the code and let the lecturer's character answer as if the students knew it themselves. Instant code debugging from a screenshot was possible by time travelling from the present to the future when the screen was available. Then, the code was debugged, and the algorithm was completed with minimal fuss. Complex recursion from within findall, subterm with address, this with a heuristic and other written or unwritten custom predicates required neuronets to find the correct code configuration. Screenshots of the main code could be supplemented with mind-read specifications and previous and saved data.
55. The immortal maintained an option global variable to switch features on or off in the source, compiled code, or running, with interaction and contraindication documentation and usage guide. Users could interact with their dictionary on the computer, and mind-reading allowed them to explicitly track their thoughts to help them remember forgotten thoughts and direct thoughts. The laptop performed the mind-reading without humans, and users could enter preferences on whether to be mind-read and track their data. The reminding algorithm modified, added and deleted specs, sometimes using previous specs or previous thoughts about specs. The directing algorithm found a list of possible features implied by the student's past work or those in the student's mind and helped them complete them one by one or the highest priority ones using mind-reading.
56. The immortal predicted that mind-reading would become mainstream to complete work and its associated tasks. Mind-reading helped finish algorithms by writing specs in conjunction with Spec to Algorithm (S2A) and a course in writing Spec to Algorithm. Students wrote the S2A algorithm as a project to explore the workings behind neuronets and to speed up development time. Mind-reading helped read students' thoughts to enter specs in S2A, retrieve lost data, find past and future possible features, organise these thoughts, develop new techniques, imagine inaccessible parts of algorithms, connect with their other ideas and keep an indispensable record of their thoughts. Meditation helped to mind-read by preparing for possible questions, where mind-reading funnelled relevant responses.
57. The immortal asked, "What is the movie about the person's robot?" The mind-reading neuronet optimised pathways to prevent the need for mind-reading made simulating the self as a robot possible and presented ethical quandaries. For example, guidelines or laws may be needed to avoid fraud, misuse or identity theft. However, the advantages of controlled use outweighed the disadvantages by providing a psychological "lab" that users could interview, learn from and improve themselves with. Still, mimicking errors the person made may lead to rebreasoning (a cascade of further) errors, and humans shouldn't pretend the robot is them when completing work, tasks, or legally binding tasks.
58. The immortal initially used another version control system and testing system to develop S2A. I added Lucian CI/CD and GitL (a version control system) at least to assess them and Mind-Reading S2A as the student wrote them. The user wrote both these algorithms with S2A and S2A with them. First, the user wrote S2A, then these programs with it and then redrafted it with them. This process of continuous refinement is repeated until the desired result has been reached.
59. The immortal returned an error on conflicting S2A predicate names. Combination Algorithm Writer (CAW) is replaced by mind-reading, but a non-regression neural network form of CAW could make autocompletion predictions. These predictions may be in the spec, formulas in the spec or over multiple specs or predicates' specs. They might consider the specs so far, user data and whether to simultaneously optimise specs or follow preferences about whether specs should call themselves or use recursive structures. A string break symbol or view of the data from the spec without variables or formulas may be used by the human or the mind-reading algorithm to edit the spec.
60. The immortal used mind-reading to develop neuronets to help debug, add features and identify and remind to correct errors in ways of thinking. A course in CAW with S2A taught the principles underlying finding the algorithm, and students still had to work out and enter dictionary algorithms. Writing CAW taught them how it worked and enabled them to train a neuronet with its data, teaching them about writing dictionary predicates and speeding up development time. The degree could be spent on tweaking specs, S2A, and other inputs, in effect training students in advanced programming techniques that minimised effort and resource wastage. Consented-to-mind-reading could be used to assess students, and mind-reading algorithms could be evaluated using verification and submission scripts.
61. The immortal saw screens of options as they were mind-read or autocompleted a spec and predicate using their previous work. I assessed Text to Breasonings, List Prolog Interpreter, Mind Reader, State Saving Interpreter, Time Machine and Immortality, Lucian CI/CD, GitL, Strings to Grammar and Spec to Algorithm using Mind Reader. S2A automatically and reliably handled base cases and recursion as part of this. Students could convert their mind-written specs, i.e. "HEADER :- BODY." and new specs for each algorithm level to specs. I drew graphs of time saved with subterm with address, Spec to Algorithm and Mind Reader.
62. The immortal attempted the exam using Mind Reader or a combination of Mind Reader and the editor. The student completed the extended algorithm in the exam with Mind Reader and S2A without using plagiarised dictionaries. They acted as a test technician, contacting a lecturer if they found a test too time-consuming. They were not allowed to use outside help, and they followed procedures to separate features to stop them from misbehaving together and tools to integrate them or to give better results for combinations of features. For example, they functionally decomposed the S2A list and string splitting optimisation algorithm using a formula in S2A rather than recursive findall.
63. The immortal stated that for the project, they provided the project specification and a FAQ and set correctness as the criterion for verification and submission scripts, which increased student performance and satisfaction. Teaching staff facilitated project guidance and debugging labs, and a group project increased the sense of teamwork and achievement when conducting mind-reading. If working online, students could download/develop the software and develop software with it. The subject dependency might be Spec to Algorithm, Mind Reader, then Algorithms, although Text to Breasonings may be needed in the first year to help with pedagogy.
64. The immortal simplified specs by eliminating variable names such as 'A' and 'B'. The student added the feature to the algorithm by mind-answering where to apply formulas. S2A asked which data structure to apply the feature to and what the feature's specs and formulas were when in Feature Mode, as opposed to Spec Writing, Testing, Applying Changes to Specs, and Optimisation Modes. After adding features and the debugging and testing cycle, including applying changes to specs to other specs, specs were automatically optimised. Stages after writing specs or adding features may be automated. All formulas could be written using S2A bottom-up using the same process or completed using a CAW neuronet, using an algorithm that identified lists, strings, parts of lists or strings taken as inputs by the algorithm and used their output elsewhere in the spec.

65. The immortal proposed that Spec to Algorithm needs to organise specs using optional or separately testable features that can be switched on or off. This fact raises the need to write each feature or code that combines each relevant feature when producing output. This point is optional for core features that don't need to be switched off. Still, for optimisations and other optional features, it makes writing computer science programs easier and modularised. This idea is feasible given that the number of different non-deterministic "feature combination" conditions isn't too many, or it is intuitive to develop them separately and combine them using a decision tree if they are independent. In the latter case, I created them one by one. I gradually increased the complexity of the code, jettisoning old combinations of features and making the code easier to edit and debug.
66. The immortal considered the S2A feature models. I simplified the first model (with different non-deterministic "feature combination" conditions) to the second model (where the features are developed separately and combined using a decision tree). I did this by keeping one feature per combination and running them one after another (deleting unnecessary features). I merged dependent features. However, having the ability to switch off as many unnecessary features as possible in the first model aided simplification and assessment of features.
67. The immortal stated that modules worked by making a feature or features available to the programs in a user-friendly manner. Combinations of features could be simplified to an if-then tree of features to streamline development. This action would naturally form a dependency of features for each point of the algorithm that could be uniformised. Permanently off redundant, unused or overly complex pathways could be removed or brought back if necessary. An algorithm could be used to optimise these pathways and reinsert code seamlessly.
68. The immortal programmed features and dependencies from predicates of specs or formulas in specs. An example of a feature is a condition of an if-then statement and its consequent exclusion of other if-then statements. Features are non-essential parts of programs, such as optimisers, that can be switched on or off for testing and use. Similarly, modules can be included in a programming language. Features can be mind-read, entered manually, or found using an algorithm or a neuronet.
69. The immortal stated the feature switches allowed easier creation, editing, and management of features in S2A. I drafted the order of running features, contributing to whether they were all necessary or others were necessary. Subfeatures may be moved, added, or deleted. I prompted programmers to meet the spec for a feature if needed. They could auto-enter or mind-write the features.
70. The immortal wrote and ran the algorithm with software that assembled the program, including switches for features and removing them if necessary. I switched off features for debugging. I wrote help and debugging features that switched needed features on. I switched the feature on by inserting the essential code that did this. This action ran the code in another predicate if the switch, stored in a global variable, was on.
71. The immortal systematically requested each predicate's features, structure and feedback on their behaviour to test them. I converted S2A specs into a feature (dependency) form for storage to reuse features. I added connectors to keep features separate and manageable. I isolated and debugged features by testing their combinations while switched on, controlled by a switchboard term in a Prolog file, changing their code using input methods or an algorithm.
72. The immortal customised a feature based on a given spec. I improved performance by turning off or deleting features. This technique may psychologically prompt one to use advanced techniques such as subterm with address because it gives one better control over development. I finished the backlogs by finishing algorithms in the notes with S2A, neuronets and feature dependencies. I reused old algorithms or developed new ones if necessary.
73. The immortal examined the mathematical theory that one discovery led to multiple other discoveries. Prolog created includes files for repositories or had different includes files for each repository's files. This action resulted from collecting dependencies and ensuring proper installation of needed files. I developed a corrector that adds files with missing predicates with confirmation. I updated installations and corrected bugs.
74. The immortal encouraged students to use Spec to Algorithm to develop algorithms to reach more advanced and complete algorithms more quickly, including their projects. In the Academy, students were presented with high-quality programs they could work on later. These programs were offered in various stages of development, sometimes none at all. The students were, however, interested in their understanding of the algorithms and started taking steps to research or complete prerequisites. I helped them collect the necessary skills to complete the projects in the subjects.
75. The immortal suggested that students might need Spec to Algorithm to build and program a robot. The robot could help with science, art, teaching, and object industries, including building computers and robots and completing combination object finder in Quantum science. Spec to Algorithm sped up development, allowing more computing, electronics and reasoning subjects to be programmed. A variant of Spec to Algorithm could "effect" an object from a description.
76. The immortal collected the spec before the formula definition or kept collecting specs until no formulas were found. I manually entered and integrated code for feature combinations with mind-reading. Rather than a single switch, the features were in an if-then tree with conditional switches. Sets of sequences of values could be made into this kind of decision tree, or formulas could take values from input and create an active decision tree that passed values along to build output. The user specified writing the features depth or breadth-first or imagined values somewhere in the middle.
77. The immortal suggested that infinite simulation depth was possible because of the universe's innate dimensions. I created a cube of a specific frequency for a quantum particle to move towards. I learned the particle's language. I examined its interaction with other particles. I recorded the conversation. I appreciated mind reading as finer-grained than the universe.
78. The immortal helped meditators pick up the courage to remain the same physical age and avoid death. I added meditators to the anti-moral end list. Ideally, there should be no risks in society, and death should be prevented. Immortal simulations should cope with the load of people. These meditators' moral ends were prevented by breasoning out eighty original sentence breasonings for them to appear at midnight each night to cover the following day.
79. The immortal claimed that Grammar Logic (GL) could quickly bulk-find and breason out details that a neuronet could parse. I modified the extended and faster meditation scripts to optionally breason out 80 breasonings to prevent death in meditators per day. They breasoned out the breasonings at the end. The breasoning occurred at the end of the extended meditation script, taking hours and needed to be optimised, possibly using VPSs. The faster meditation script theoretically breasoned arguments using advanced technology for each meditator, expressed similarly to breasoning out the "Quantum Box" stamp.
80. The immortal stated that S2A involved entering specs or formulas and was easier to develop in because base cases and recursion were automatically handled. The suite of algorithms, including Spec to Algorithm and the neuronet, reinforced learning by testing humans on the computer science involved. Some questions were cutting, while some used Spec to Algorithm. The academy didn't necessarily use neuronets but used S2A, Mind Reading, and possibly CAW for simple but time-consuming formulas. More difficult questions tested knowledge of hard bugs, the effects on many CAW code results, and debugging many variables in the interpreter.