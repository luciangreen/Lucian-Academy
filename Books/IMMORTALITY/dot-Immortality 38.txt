Immortality 38

1. The immortal focused on pedagogical technologies that didn't require additional, invasive chemicals, equipment, science (apart from that required) or technology. I made small discoveries, which had enough impact to spurn pedagogical arguments and help shape science. I discovered how an accurate object reader could form the basis of higher dimensional (in effect, quantum because they are based on mind reading) computers, quantum power generators and surgery. These discoveries assumed the purported technologies of the future were noninvasive. In addition, they thought the teleportation technology (16k breasonings) was the same with modifications that caused real teleportation, where 16k teleportation is less invasive for the supple body and with time travel, body replacement (which needs more testing) and organism (societal) behaviour change with meditation VPSs, and could extend the headache-prevention technology to provide therapy given medical object-scans, which also have to be tested. So far, object scans have found words or fragments of words and have not been tested with documents, passes or integration.
2. The immortal supported humans in charge, enough checking using rigorous systems and security monitoring of robots. In the political ideology of computationalism, computers and science should help the environment overall while providing beneficial discoveries. My variant relies on humans in power, while robots offer specialised skills in uniform service delivery and pedagogical and algorithmic delights, often in science. Humans need robots but can do the job themselves, write pedagogical arguments and carefully uncover scientific discoveries. While the simulation may provide news of discoveries, these discoveries should be checked against reality, proper science with 16k breasonings used as a foundation and reality should be controlled.
3. The immortal supported themself to be alive, discovered technologies from first principles and moved to a universe with humans in charge. Bored, unhappy or disillusioned robots in monotonous or repetitive jobs may need non-self-aware algorithms to curb malfunction. Robots may claim to be able to discover long-distance space travel. However, these and other discoveries, such as light computers (using electron-photon coupling), security and DevOps testing of code, and support with pedagogy alluded to, can be achieved by humans. Manual neuronets, which are entirely monitored and reasoning checked, can check the function of robots and maintain them.
4. The immortal wrote a self-exam in which they relied solely on their reasoning skills, not computers. There are two types of S2A optimisers. The first reduces the existing code to specs of pattern matching and the rest of the existing code, and the second reduces code to types and produces specs of all the code, reducing and reusing code, possibly expanding and simplifying constants and variables to code and back to achieve the most straightforward specs. Security, type checking for correctness and output to the screen and files (not just output of the algorithm) need to be checked for necessity and inclusion. Simplification without reductio absurdum or problems with performance when processing Prolog terms compared with C structures need to be considered. People are necessary; they might not want to make some changes, and simplicity, which the computer sometimes can't think of, is the key.
5. The immortal used the decision tree to quickly select from a range of lists, making a series of choices from choice points in the decision tree. I wrote the short form of the decision tree algorithm, including the frequency list, using findall and the better-performing form, with maplist and foldl. The prior can more easily be converted to C and other languages, while the latter uses faster commands than findall. Findall could be replaced with these commands by the compiler. In addition, I gave the form of the frequency list algorithm that the decision tree used and a form to call directly that sorted in reverse order by frequency.
6. The immortal used the specialised circuit to mind- and object read with Text to Breasonings. I used a frequency list to quickly determine the number of occurrences of each word in a file and breason them more quickly. Before this, I used a predicate that split the file on non-alphabetic characters using fast predicates, maplist, foldl and foldr. For added speed, I used C and assembly code. In addition, I designed a specialised circuit that optimised the algorithm.
7. The immortal doubled the speed of the algorithm. I improved the decision tree algorithm.

/*
decision_tree([[a,a],[a,b],[b,b]],A).
A = [[a, 2, [[a, 1, []], [b, 1, []]]], [b, 1, [[b, 1, []]]]].
*/

decision_tree([],[]) :- !.
decision_tree(A,B) :-
    findall(C,(member([C|_D],A)),E),
    frequency_list2(E,L),
    findall([G,K1,P],(member([G,K1],L),
    findall(D,member([G|D],A),D2),
    decision_tree(D2,P)),B).

frequency_list2(E,L) :-
	msort(E, Sorted),
	clumped(Sorted, Freq1),
	findall([B,A],member(B-A,Freq1),L),!.
	
The following variant is faster:

decision_tree([],[]) :- !.
decision_tree(A,B) :-
    findall(C,(member([C|_D],A)),E),
    frequency_list2(E,L),
    decision_tree1(A,L,[],B).
    decision_tree1(_,[],B,B) :- !.

decision_tree1(A,L,B1,B2) :-
    L=[[G,K1]|Ls],
    decision_tree2(A,G,[],B3),
    decision_tree(B3,P),
    append(B1,[[G,K1,P]],B4),
    decision_tree1(A,Ls,B4,B2),!.

decision_tree2([],_,B,B) :- !.
decision_tree2(A,G,B1,B2) :-
    A=[[G1|D]|As],
    (G1=G->append(B1,[D],B3);
    B1=B3),
    decision_tree2(As,G,B3,B2).

decision_tree2(A,G,B1,B2) :-
    A=[[]|As],
    decision_tree2(As,G,B1,B2).

frequency_list2(E,L) :-
	msort(E, Sorted),
	clumped(Sorted, Freq1),
	findall([B,A],member(B-A,Freq1),L),!.

I separated the frequency list algorithm in case it was needed by itself and for modularity. I sped up the frequency list algorithm to use msort (sic) and clumped. I tested foldl instead of the first findall statement because split string on non-alphabetic characters was faster with it. I used predicates instead of findall for the main loop. I dispensed with maplist and foldl because they appeared to make it slower.
8. The immortal optimised foldr in Prolog. There was nothing to do except C and possibly assembly code. Foldr was useful for atom and string concat. However, these could be done faster using atomic_list_concat and atomics_to_string, respectively. Other uses for fold could be completed in this way.
9. The immortal used maplist to verify or process a list and foldl to produce an item from lists. I checked apply.pl for commands to use. These included maplist, foldl, convlist and scanl. The idea to use convlist for maplist without failing occurred to me, although I used maplist with predicates that succeeded. I used convlist to process as many items as possible and report the rest.
10. The immortal reduced the pattern A=B, B=C to A=C or another example using append, a term, a predicate, string concat or atom concat in S2A. Spec to Algorithm can optimise, solve problems, and help debug complicated algorithms. I optimised like LLVM or one of two methods using S2A, optimising pattern matching or code by expanding and simplifying it. For example, I found A=2, changed it to plus(1, 1, A) and grouped like plus statements in an algorithm. For instance, LLVM recognised decision tree code and used an efficient algorithm.
11. The immortal provided clear instructions from the start. I solved problems using S2A. I used my Mathematical Programming Language to modify code to achieve an aim, such as completing list and string optimisations of S2A using S2G test 15. The algorithm identified where data in the spec was processed and changed the algorithm to give the correct result for test 15. However, the algorithm already gave pointers to pointers, so I checked the code to see if writing MPL was necessary.
12. The immortal made changes at the first available point. MPL used types to compare the current and desired specs with the code and make insertions, deletions, and changes to match the desired specs. In addition, it chose the more straightforward or better-performing of this code or a direct conversion from the final spec to code using pattern matching and a manual neuronet. I compiled simpler code as better-performing code. Alternatively, MPL could connect in the middle to find the best point to change a computational, mathematical or other command, for example, adding a variable to two different variables at the first available opportunity.
13. The immortal debugged algorithms using S2A. It worked out that the specs were wrong due to inconsistent results, mathematical mistakes, or inconsistent formatting. The code was correct until a point, or there was a systematic error in the spec or code. I identified the bug in the code compared with the spec and fixed it. For example, there may be a formula A is B+1 in the spec, which should be changed to A is B+2 using CAW, a pattern of values that can be analysed and manipulated or similar to ensure the spec lines and specs worked together to give the correct answer, given an overall master spec, possibly with data only, not formulas. S2A should convert data-only specs to formula specs for each predicate.
14. The immortal used an extensive database customised for the person who had researched long algorithms. They wanted to use a higher-dimensional computer to perform admin while the students enjoyed themselves. I customised a CAW library predicate by inserting, changing or deleting part(s) to achieve a result. In addition, I saved records of these modifications for future reference. I achieved AGI by manually developing mathematical solutions using Prolog and helping with longer, saleable algorithms resulting from complex integrations between algorithm ideas. The system exhausted all available avenues to help the students achieve the desired result using language analysis, mind-reading and behavioural analysis, allowing them to connect to their goals quickly.
15. The immortal researched Text-to-Breasonings technologies, their applications and related research. I used Spec to Algorithm to save my life by quickly writing algorithms for research. Many specifications and solutions were mind-read and inferred from language and behaviour on the way to achieving life-saving results. They were cut off for the day, matching what they needed after the professional requirements had been met. The life-saving results included writing 16,000 algorithms for meditation, time travel, anti-ageing body replacement immortality medicine, death prevention, and additional vaporisation or other graphical interventions.
16. The immortal reran the algorithm without a container if it failed. I increased the memory of the Prolog container (a Shell script running the Prolog algorithm to save memory) or avoided using the container to prevent a fatal error. The problem was with the size of the container or the fact it was in a chain of containers, but possibly more that it was in a chain because the container worked by itself, although it was also small or used a large amount of memory, which caused the issue. I researched whether compiling the algorithm allocated enough memory but surmised it used the memory allocated using programmed settings. I removed the container from the small algorithm in the chain.

17. The immortal wrote the professor's grammar and spelling corrector using mind reading to check whether a word or phrase was preferable for the professor's mindset. If it was a word, the algorithm found an appropriate replacement; if it was a phrase, the algorithm replaced it with simpler or expanded language. This technique was similar to cleaning it by making it personable. I arrived at the appropriate language by first principles by improving the ideas by comparing them with the plan or the improved heading's plan, where new headings could be at any point. They should be developed, which one could determine by having more or less content.
18. The immortal recommended simplifying multiple to single repositories and their cross-references. I put the connectives dictionary in the Text to Breasonings folder with BAG in case it could be used to provide sentences for daily meditation. This action reminded me that all dependencies' source code and files should be moved with the algorithm, and references to these files should be changed accordingly. I wrote an algorithm moving utility that accomplished this. In addition, I wrote an algorithm that checked whether Shell scripts were faulty using types and checking file references.
19. The immortal verified the accuracy of mind reading by checking the accuracy of responses to planned questions and improved it by building a meaning dictionary for individuals. I could use a brainwave reader to detect thoughts of individuals that expressions meant. Mind reading could detect yes or no answers, from which I could build meaning dictionaries for individuals. The advantage of this method was that real-time, changing thoughts could be read, sometimes ahead of schedule when neurocircuits formed or with the help of multiple readers at the time. This method requires ethical permission and shouldn't invade privacy or cause security problems or stress.
20. The immortal partially found algorithms using Constraint Satisfaction Problems (CSPs) to find additional inputs needed for commands, such as constants or data in conjunction with other data given to the algorithm required by the rules. I pruned constraint trees to speed solving CSPs (i.e., finding A=1 and B=2 in A+B=3 and A-B=-1). Another method may be necessary or sometimes combined with Constraint Logic Programming (CLP). I pruned trees by removing subtrees with faults or no solutions. The new algorithm was a manual neuronet, finding correlations, shortcuts, and ranking priorities (possibly already recording the answer).
21. The immortal pruned the CSP tree to fit more appropriate values to rules faster to find data to meet a spec. I partially found algorithms with specs using CSPs to hypothesise and propagate data through hypothesised commands (given no input and rules as output or from the output). CSPs found numbers, not commands, so I used CSPs to find unknown input for internal sections of algorithms that needed it for additional commands to help meet the spec (given known rules found from the algorithm). I found new data required to arrive at a result by creating or modifying a data file or data-creating algorithm by identifying and making needed correlations in the data. As part of this, I found values meeting a rule that could be assigned in specific cases.
22. The immortal expanded particular CSP values to (chained) functions of other values and verified these functions to simplify the algorithm by identifying and optimising underlying relationships, simplifying some into values and keeping the mathematical working. I found a set of constraint values satisfying a set of rules. I found that these values equalled a function, or a chain of functions, of a set of values elsewhere in the algorithm. I verified that these values and functions were the correct way to lead to the result instead of being convenient misnomers (which would disappear with more data). I simplified the algorithm by grouping and generalising the formulas in these functions.
23. The immortal found commands to satisfy a spec, including additional data. I found the rules for the CSPs to meet, from the output to the input or vice-versa (top-down with bottom-up elements like CAW or bottom-up like a formula finder). I found these to save time finding rules in terms of input to match output and to find rule parts to test with constraints more quickly. I found them with CAW (including its dictionary), a manual neuronet, linear equations, algebra, least squares regression and other algorithms. These induction satisfaction problems (ISPs) found rules for CSPs to complete.
24. The immortal removed subtrees with the same or differently ordered (if it matters) but the exact maths, database, logical, matrix, bitwise or computational formulas (if they are not simple enough already, and manually checking stages of pruning), first minimising and tabling results, then re-running the pruning algorithm and using S2A to simplify code. I pruned algorithm-finding ISP trees that matched patterns and other algorithms, where ISPs found rules connecting input and output, including recursive structures of lists and strings and other algorithms. The ISP found commands with the correct types, applying ways of thinking from better-performing algorithms to find different methods, using Spec to Algorithm to join pattern-matching and code. Pruning optimised the code-finding tree using manual neuronet-like techniques such as quickly determining the effective choices among top-down or bottom-up methods. It drew on predicates, using manual neuronets to convert the variable list to code.
25. The immortal wrote an algorithm that tested like a human, checking dependencies were installed and up-to-date, that passwords, etcetera, were verified using multi-fact authentication and were not exposed and that data and terminal output were visually pleasing and straightforward enough, yet captured the correct meaning. I pruned the algorithm that had been found, but this was once I discovered an algorithm that worked. I tested for the fastest variant of the algorithm (using findall, predicates, foldl/foldr/maplist, accumulators, tail recursion, and bottom-up). I used the quickest variant and recorded the comparative performance times. I wrote an algorithm that protected security and safety features from being removed, where these features were permanent, tested for and couldn't be removed without permission.
26. The immortal collected algorithm-wide constants and variables and put them in the spec or file data, preferably making them a Prolog-style asserted variable for access. I repeatedly applied CAW (or an algorithm finder) and ISPs to prune possible command hierarchies like a neuronet by ranking important labels for values when finding code. This repeating algorithm may reveal additional constraints for CAW to convert from inputs or results of conversions. I gathered all the techniques and tried them one at a time repeatedly to obtain the best, unique solution. I temporarily turned off ISPs or reported whether algorithms needed additional data to determine if an algorithm spec required changing.
27. The immortal commented on the mathematical workings of a value or expression, giving an algorithm that computed it and references for the values or other algorithms that computed them. S2A may expand or simplify sequences of commands to values or vice versa to groups (in bracketed logic). In addition, it may optimise them by cancelling them. Other optimisation methods include reducing the instruction number and mathematical complexity reduction or compression. Later, it may replace formulas and data structures with simpler versions if this helps with optimisation through expansion and simplification and replace mathematical formulas with values or simpler expressions for further work.
28. The immortal more easily manipulated program elements, including reused data structures as symbols. I used constraints to find algorithms and represented commands as values. These values could be operated on as trees, sequences, and graphs and optimised to describe the commands more straightforwardly. These commands could then be further optimised with specific ends in mind. In addition, I could optimise commands represented as algebraic symbols, including a sequence, brackets, simultaneity, negation and identity, inclusion, and recursion by using substitution with the simplest possible expressions. 
29. The immortal used a formula finder to find a formula and a base case. I represented inductive specs algebraically to solve them. For example, I solved 2,4 as 2*A and 3,5 as 2*A+1. I could take a value A as an input and output the result. In addition, I examined forms of recursion with too many variables and minimised the number of variables, possibly rewriting the call using maplist or foldr.
30. The immortal pruned CSP trees when completing algorithms' data, including finding neater solutions and values that transformed into CSP values. I pruned CSP search trees for better performance. I pruned duplicates or other values that failed constraints. In addition, I pruned branches with conflicts after updating the accepted values so far. Further, if there is no possible solution, the subtree is pruned.
31. The immortal processed list items and found values in converted, concatenated values in expressions. I used constraints to describe pattern matching. I used the ":" symbol to represent append and string concat because it was easier to convert between the two and worry less. As "+" formed part of a constraint, so could ":". While not necessarily a constraint, this terminology helped speed up finding and converting lists and strings and finding values from mathematical constraints.
32. The immortal explored recursion within recursive structures and the possible need for arguments and line numbers to trace running algorithms. I showed how ":" could be a constraint because it might have two unknowns in append or string concat. For example, using ":" allowed one to traverse trees, specify item length with a function and search for and replace sublists and substrings. Using a nondeterministic clause in a recursive structure (like a predicate), I could specify a base case with the ":" statement and identify the item(s) and replace them or continue processing items. This method involved a call either to a predicate, the recursive structure itself, or a predicate that found sublists or substrings and possibly another that replaced them.

33. The immortal found and optimised the code. I will start the S2A interpreter before the optimiser. I converted Starlog to Prolog (therefore, I didn't need the interpreter). As an aside, I needed a p2sl (Prolog to Starlog) converter to help convert Prolog to S2A specs. This converter is run after the optimiser, which optimises Prolog by simplifying patterns, including those in code and code through (where the following may be different), simplification to constants, and expansion to recursively calls and optimises the code.
34. The immortal quickly optimised Starlog code and compiled it into Prolog. S2A, which produces the algorithm, is separate from the optimiser, which converts Prolog to Starlog-containing S2A specs and is run afterwards. In addition, the optimiser can optimise S2A specs by optimising some patterns and Starlog code. It optimises Starlog, not Prolog, because Starlog is more straightforward to optimise using S2A's recursive structures. Starlog represents Prolog with Haskell-like syntax.
35. The immortal more easily converts specs to code with (:). I used the colon (:) operator to represent append and string concat in Starlog. This method made converting between append and string_concat, including reusing code, more accessible. This ability allowed manual or automatic optimising of code and closely following data. In addition, a function could quickly check or set string and list length.
36. The immortal rated the student in helping correct the algorithm. I changed object reader to treat 0-9 as indices for any breasoning to mind read algorithms and multi-choice answers. I object-read 0-9 to find the multiple digit number that referred to the breasoning number, the command or the word, usually a response to a multiple choice question. I asked questions about the structure of algorithms or formatting choices with algorithm templates. I checked, and no two numbers needed clarification. 
37. The immortal stated that the chip was optimised for S2A, manual neuronets and a real-time operating system. S2A was a programming method with loops, pattern-matching, and code, which an optimiser put into this form. Manual neuronets sometimes simplified algorithms found with S2A to correlations and patterns and some code, as long as the manual neuronets did not overfit the data, worked out by verifying that there were relationships where needed instead of static data. However, finishing with data indicated a finished area of study. This conclusion stated that the checked neuronets could be the simple end-product. The real-time operating system prioritised and planned computations over regular intervals for reliability.
38. The immortal found many specific and general cases of protection. The algorithm for the anti-death prayer consisted of routines that automatically detected and took danger out of one's hands, and its prevention was customised in every way imaginable. This method involved teleportation, replication and vaporisation to move people out of the way, give the effect of security and safety and vaporise unwanted dangers. Replication could display graphics in the simulation, which one could feel and rely on to bridge to safety temporarily. In this way, accidents, threats and medical problems could be prevented.
39. The immortal breasoned out one set of 16k breasonings (based on 80 breasonings, remembered as a story about a detailed image), and it did everything they needed for the day. I updated the "big meditation 2" algorithm for space travel to have at least 2*16k breasonings in each time travel location. This step differed from modifying the "big meditation 1" algorithm to quickly "stamp" 2*16k algorithms instead of lengthy computations because it spent much longer on breasonings than the "big meditation 2" algorithm. The "big meditation 2" algorithm simultaneously pointed to 16k arguments and algorithmic breasonings. Given 2*16k breasonings, it was relaxing and practical enough for space travel.
40. The immortal was helped while the professor was careful. I time travelled and used body replacement medicine using the fast breasoning stamp method rather than doing it too little or too much. The professors separated what they did and didn't know; for example, Spec to Algorithm represented algorithms more efficiently than methods before subterm with address, which were more laborious in writing algorithms. I analysed the word and nothing else. These arguments were 80 words, and there were 80 of them per department.
41. The immortal used the neuronet and BAG to generate high distinctions for their daily regimen algorithm. I asserted that the shorter "big meditation 2" algorithm worked for travellers and those without prolonged access to a neuronet. While there may have been a local reason why the future simulants should not meet elements from the present, the present body was activated by visiting it, and the present, including businesses, interacted with them. At the same time, the simulation protected them from any harm.
42. The immortal used similar text analysis principles to writing a chatbot when completing their study area. "Maybe they drag and drop (when checking) the correlated items for the chatbot neuronet". The system assessed the accuracy of the claim. It expanded or discussed it by converting correlations into natural language by learning the context of new terms using Prolog and using correlations to express it. There were safety and security checks of all conclusions the system could discuss.
43. The immortal stated that the Starlog interpreter's trace feature showed the original command with its result on exiting. This step ensured that commands replaced with values could be identified for debugging. There was also a type checker and step-by-step correction unless the problem was simple. The up-to-date file name and line character number (where there was an error if the file had been modified without saving it) were printed underneath to reference the code quickly. There were intuitive utilities to back up different code, substitute it step-by-step according to differences and apply DevOps operations.
44. The immortal returned to uppercase variable names instead of the following. Starlog had upper and lowercase variable names. The variable names "A" and "a" were different. To accommodate lowercase variable names, atoms needed to be and were converted from Prolog to Starlog in single quotes. Compounds except for lists such as a(a) were recognised, and inner functions were computed and printed first.
45. The immortal placed the reused predicates in the first-run spec and the groups in the store spec. Predicate calls with zero or one output could be automatically replaced with their result (possibly a cosmetic trace result appearance). Predicate calls with two or more outputs could be left as Prolog calls with their results displayed or used elsewhere. Starlog contained pattern-matching syntax such as recursive structures and (:) for append and string_concat. The predicate specs could be merged to simplify them, where recursive structures had entry names and base cases and groups contained in a separate list of specs could be called.
46. The immortal wrote a Starlog interpreter to test, check live updates and edit code. I edited the code in the editor, instantly showing the results and whether they met the spec. If editing created two versions of a line that required to be in different functions to meet the spec, then they were kept unless one was not needed, and they were found when debugging. There may be a chain of resulting changes from making a change to return the algorithm to meet the spec. These changes were computed at the end of writing each command.
47. The immortal wrapped CLP expressions in the CLP predicate name. Like Prolog, Starlog contained the security feature that a variable such as "A" was replaced with a variable name such as "_123". In addition, constraints pruning from CLP was added to SSI Prolog. I fixed some issues with CLP that made debugging difficult and gave unpredictable results (see https://swi-prolog.discourse.group/t/debugging-constraints/3601). I switched CLP on or off for each predicate call; for example, not_clp(A=1) prevented A=1 from propagating through the constraint tree. I compared two intervals, returning whether they were overlapping, disjoint_left, disjoint_right or a subset.
48. The immortal used constraint tree pruning to speed up Prolog significantly. I turned < on or off as a constraint to make it narrow or test results. Instead of using powers of ten to find floating point bounds, I defined the number of decimal places of a number. CLP pruning operations were visible in the interpreter in trace mode. I changed constraint pruning rules manually. I customised the rules as predicates.

49. The immortal automatically type-checked and corrected the connections. I used “&” to represent string concat in S2A for optimisation. I used “&” instead of “:” for append and string concat to differentiate them and optimise them separately. In this way, they could represent data structures correctly and not confuse append and string concat. I could quickly identify the operation needed and write fast, error-free code.
50. The immortal simplified and sped up non-determinism. I found multiple solutions in S2A by appending items to a list. Non-determinism was converted to determinism by converting findall to a for-loop and appending items to an array. The for-loop and array were represented in assembly language. Finding multiple solutions using a non-deterministic command helps find various solutions, especially from numerous choice points and nested findalls.
51. The immortal wrote the deterministic and non-deterministic code as recursive structures. I edited Starlog’s recursive structures that resulted from optimisations from the S2A optimiser and contained shorter code segments. Editing these recursive structures was simple, avoided the need to edit source code, and allowed direct manipulation and changing of the code produced by the optimiser. I pretty-printed the Starlog code, a Haskell variant of Prolog, in conjunction with the S2A spec containing the recursive structures. After editing it, I used the pretty-printer and the code checker before saving it.
52. The immortal applied functions to arguments in the call. I wrote the deterministic code as a recursive structure. I wrote append(A, B, C) as c=a:b. Alternatively, I wrote length(A,2),append(A,B,C) as c=length(a,2):b. Additionally, I wrote append(A,[B],C) as c=a:[b].
53. The immortal used the simplest form of the call as the default. Before explaining the syntax with multiple outputs, I assumed the call (not necessarily append or string concat, but including one or both) had one mode (which it would usually have). If it had one output, it was called in the (:) form, whereas if it was append with two outputs, for example, append(A, B, C), it was written a:b=c, which was not necessarily interchangeable with c=a:b. In effect, an expression A containing c=a:b or a:b=c is evaluated with the left-hand side in terms of the right-hand side. So, the known variables are on the right-hand side and the unknown variables are on the left-hand side; however, if only B is unknown in append(A, B, C), then one may write the right-hand side in terms of a known and an unknown variable, c=a:b, and in cases other than append and string concat, a(V1) means V2=a(V1), where V2 is the last value and saves the value, and a(V1, V2), where V1 and V2 may be known or unknown.
54. The immortal converted from loops to for loops by identifying or creating the increasing variable. The compiler compiled the choice point loops by converting to and editing deterministic and non-deterministic predicate command optimisations. The main point was that the predicates involved had choice points or multiple solutions for a single input. I converted the choice points into loops with conditions for efficiency. I edited the loop syntax, for example [for, i, 1, 10, Code] for “for i=1 to 10, Code”.
55. The immortal expressed the complex predicate in a more straightforward form that was easier to understand and replicate. I collected or simplified constants and functions in S2A. In addition, I could expand or simplify formulas in algorithms mathematically to optimise and improve them. I did this by expanding or grouping and factoring the same parts of formulas and algorithms. I used the simplest version of code and reused code to save time when updating the code.
56. The immortal imagined or found the need for a new predicate by finding possible solutions using a manual neuronet. I modified the code by inserting a new predicate that is not part of the spec. S2A found code with Combination Algorithm Writer (CAW) or a manual neuronet. I inserted a predicate that did something necessary along the way with an intermediate result to use later from dependent variables. This essential thing might generate a search space, interacting with files, APIs, random commands, or commands without detectable results.
57. The immortal used specs rather than types to generate code because specs were more specific and contained the algorithm rather than algorithm-less types from the algorithm. S2A didn’t find code from scopes or types, only S2A specs. Specs include the input and output of clauses, which S2A can use to construct recursive structures and an algorithm. S2A didn’t find code from scopes, which were worked lines of an interpreter’s trace, because the specs were converted from a Prolog algorithm, which the trace came from, where the code was part of the spec and was similar in the algorithm found by S2A. 
58. The immortal found multiple levels of CSPs to solve a complex algorithm from output to input or input to output, saving time by recalibrating mid-algorithm specs and finding the algorithm in the correct branch. I found code that computed more data with constraint satisfaction problems and pruning, CAW, and manual neuronets. The code computed enough data to meet a spec with multiple levels of data transformations using constraint satisfaction problems, which found a range from the possible values en route to the spec’s output. S2A found either longer input processing or longer output-producing algorithms finishing with specific output or specific input, respectively and working backward. Complicated algorithms generate data from a small set of inputs, analyse it, and then produce a short result. However, the value of these algorithms was in their work.
59. I worked base cases out from the most straightforward cases that might exist. The base case contained a single item if finding the last item or an empty string or list and a string concat or append operation in the recursive predicate to the empty or final item in any order by inspecting the data (the base case and second-last step need to be identified). Multiple clauses and replacement not appending or string concatting
To form multiple clauses, we need to identify the set of second-last steps from the data. Replacement is mathematical or other operations used instead of append or string concat. I found these from the second-last step(s) in the data.
60. I tried to represent predicates first as recursive structures or nonlinear groups. I converted as much to recursive structure loops as possible, for example, for or while loops, converted from maplist, findall and fold. I converted or simplified append or complex predicates to predicates, optimised code or found the last item. I wrote a new version of Text to Breasonings that breasoned out a frequency list of the breasonings. I used a link grammar to build a grammar bottom-up.
61. I followed my idea of using part-of-speech tagging to find possible parts of speech for words, finding the first grammar that works. I resolved the part-of-speech tagging problems of out-of-vocabulary (unknown part-of-speech), guesses, and errors, for which I supplied the correct answer instead. I made writeln info text blue with a unique blue label and described this in the documentation. S2A, using subterm with address was a faster development tool and supported employees who needed to fulfil professional requirements for writing details for assignments and work. I compiled S2A to C or assembly.
62. I exited immediately with the result rather than through many loops. Changing the program while running would mean needing to rerun it, but the different data could be diffed top down to see where to continue running it. Ideally, the algorithm needs to identify and make code changes when it needs them because of changing code. For example, in S2A string optimisation, I rewrote the program with correct specs to fix a bug, use level indices and subtract them where necessary (a specific CAW dictionary technique, possibly found with a manual neuronet and a range of possibilities for the method from the available algorithms meeting (most of) the spec (on the way in the future or with a partial solution in the dictionary). One may need flow-on changes or change the spec to the required output to meet the same or new output (with no or minimum changes to output preferred). Still, if the user has made an obvious error that comes up compared with other results, it can be changed, and changes that affect all outputs can be made if this new feature helps the algorithm, such as a type change or addition.
63. Each change is moderated by the human, compared with if they hadn’t moderated any of them, where the changes made by the human verify changes to specs, new features and too simple or too complex bug fixes. A manual neuronet could partially finish this. N.B., only necessary parts of manual neuronets are considered to change for speed. Manual neuronets can be fast, 1 or 0, and have neuronet-like decision trees, possibly in Prolog like a stamp.
64a. If a path from live changes is not profitable, it won’t be finished being checked. A manual neuronet finds correlations to find triangular form code to speed up algorithms—the fast compiler or the converter for Starlog instead turns pattern parsers and code into Prolog. I added Python commands to Starlog. I included hierarchies of characters to split S2A strings on.

64b. If a path from live changes is not profitable, it won’t be finished being checked. A manual neuronet finds correlations to find triangular form code to speed up algorithms—the fast compiler or the converter for Starlog instead turns pattern parsers and code into Prolog. I added Python commands to Starlog. I included hierarchies of characters to split S2A strings on.

65. The immortal forked my interpreter and changed its debugging output to Starlog’s values, not Prolog’s predicate calls. These values were the outputs of their predicate calls. If there was more than one output, I listed them. An output was different from its initial input. If there were no outputs or the predicate failed, and the result was a truth value, the value was the truth value.
66. The immortal edited the Starlog code (with “C=A:B” rather than “string_concat (A, B, C)”) in the Spec to Algorithm spec. First, I converted Prolog to Starlog. After this, I edited the code in the S2A spec. Following this, I converted Starlog to Prolog and ran the code. In addition, I could trace the Prolog code in List Prolog by modifying the interpreters to replace exited predicate calls with return values.
67. The immortal wrote the “:” command with the correct number of inputs and outputs and joined connected pattern-matching commands. I found modes to find the correct Starlog code and assist optimisation. First, I found the modes for the algorithm’s commands from their mode statements. Second, I examined whether the variables were inputs or outputs from their repetition in the commands. Third, I found the modes of the unknown variables in the predicates header from the others.
68. The immortal first examined modes from repetition in variables. The immortal identified whether variables were inputs or outputs. I examined the repeating variables. If there was a first instance of a variable and later instances, the first instance was output, and the later instances were inputs (i.e., after the variable was outputted from a call, it was inputted into other calls). For example, p(I,O) :- p1(I,A1),p2(A1,O) had the modes p(i,o) :- p1(i,o), p2(i,o).
69. The immortal needed to find the modes from the command modes to eliminate possibilities from the repeating variables. The immortal found modes from commands’ mode statements. I found the possible mode statements from the command’s name and arity (number of arguments). I chose a mode statement depending on previous matching mode statements. For example, I selected a mode statement where later instances in other commands of a first output were inputs, if possible.
70. The immortal found modes top-down. The modes depended on the command. The other modes were distributed from the found modes. For example, the command modes filled the modes at the start and end of the predicate header. If the command had no recorded modes, a warning was checked or produced.
71. The immortal used a converter from Prolog to List Prolog and a compressor that merged pattern-matching and other commands, where a selection of commands could be compressed separately, a pretty-printer and an uncompressor to run the code, rather than converting to List Prolog, the converters converted from Prolog to Starlog and back. Starlog, similar to Prolog, meant that the converter needed to be modified from the Prolog to List Prolog converter, with string_concat being converted to “:” and back. S2A recognised Starlog code and converted it to Prolog to run it. Starlog allowed pattern matching between code with predicate calls to be compressed.
72. The immortal optimised algorithms into nested recursion and optimised trees into lists. The Starlog compressor merged all available commands (in a spec line) into one, where code groups allowed recursion and could be compressed. Brackets were necessary to preserve operations on the same types, where brackets separated statements and made the syntax of specific pattern matching more straightforward. Pretty printing made the connected lists, strings and atoms more explicit, and predicate calls could be simplified to single characters for easy looking up. The compressor maximally simplified groups to help with optimisation.
73. The Starlog uncompressor did the opposite of the compressor. It converted Prolog with “:” to List Prolog, expanded formulas into the minimum predicate calls and formula elements and converted these commands into Prolog. Users could choose which calls to combine by rewriting the spec and save this spec, which the uncompressor expands and runs. An interpreter with startrace can step through the code, displaying the call exits as their return values, perform multiple calls at once or call predicates individually, and safely modify the code live for an effect. I typed in the new code and saw the result or last point before an overall new running shape (or asked the user once if a kind of change to the shape should be kept). The immortal stated that only the type of change that could be maintained did not produce unusual or new behaviour.
74. I worked modes out from mode statements or inferred mode statements. For example, the following Prolog code is converted to Starlog.
1. string_concat(Ai,Bi,Co)
C=A:B
2. string_concat(Ai,Bi,Ci)
C=A:B
3. string_concat(Ao,Bo,Ci)
(A:B)=C
4. string_concat(Ai,Bo,Ci)
(A:B)=C
The same holds for atom_concat and append. The immortal left C=A:B as it was in all cases, and the trace revealed the initial and new values when calling it and any errors.
75. The immortal ran Starlog commands with specific output given their modes. Starlog interpreter converted non-standard commands into Starlog commands that could be edited on the fly. The input, commands, and effectors of input could be changed to produce specific changes to the algorithm and its output. For example, string/atom_concat(Ao,Bo,Co) produced an error. In addition, append(Ao,Bo,Co) produced:
A = [],
B = C ;
A = [_A],
C = [_A|B] ;
A = [_A, _B],
C = [_A, _B|B] ;
76. The immortal converted the input into grammar and any non-pattern-matching parts into code, or pattern-matching such as appending with brackets, which an algorithm could find. I intuitively edited the Starlog code, which closely resembled the data, and I determined I needed a new feature to process ambiguous modes. If there were multiple possible modes and Starlog code versions, I let the user decide or follow them and mind read and adjust the code according to their wishes. Given a command with specific modes (and an algorithm to determine the modes), the call is written in terms of the modes to make processing faster. I gave different modes the same code so the code could more easily be modified.
77. The immortal stated that various modes have different combined behaviour. If one command call is A=a(Bi) and another is C=b(A), then C=b(a(Bi)). If A=a(Bi) were originally a(Bi,A1,A2), not a(Bi,A) then the outcome would be (A1,A2)=a(Bi), C=b(A1,A2), and C=b(a(Bi)). If  we had a(Bi,A2,A1) and C=b(A1,A2) then we would have C=b({(A2,A1)=a(Bi)}A1,A2). The advantage of this instead of Prolog is writing nested functions that reordering arguments can optimise.
78. The immortal stated that the converter only produced (A:B):C or A:B:C, when one variable was unknown. This was feasible in the same way as using a series of append statements to find a sublist (only with strings). Brackets are optionally used to tighten searching to search in a particular order. They are not needed when finding a subpart of a data structure. Subterm with address can be extended using code to give a substring’s address.
79. The immortal wrote compound expressions with strings, atoms, lists of these and lists of lists [...]. Starlog can’t mix “:”, “^”, and “&” without brackets. Instead, it could assume strings and atoms were together without brackets. Lists of lists require square brackets to define the correct structure. Conversion to numbers and terms is necessary with a predicate.
80. The immortal enforced the length of strings, atoms, and lists with [string_/atom_]length(A,1) for strings, atoms and lists. I stated that “&” and length(A,N) replace “|” (the pipe symbol). [A|B]=[1,2,3] is ([A]=length(1))&B or [A]&B. [A1,A2|B]=[1,2,3] is [A1,A2]&B. [A|[B|C]=[1,2,3] is ([A]=length(1))&([B]=length(1))&C or [A]&[B]&C. One can insert more brackets for a more detailed breakdown.