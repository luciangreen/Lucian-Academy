Simulation 4

1. Projects with 100% io, spec, faq. Do they quickly recover with minimal prompting? Do hints lead to correct completion until 100% understanding, notwithstanding disabilities?
2. Robots’ inability to think in our timescale would be a disability they would have to overcome. I made a script to test whether what was uploaded worked. I read the paragraphs and expansions aloud to check that they made sense. The philosophy was stream of consciousness (run-on single-topic prose). I inserted argument words into the answers.
3. I autocompleted Starlog programs. I use a password terminal algorithm. I redid manual neuronets myself. I use a manual neuronet repository. I use complexity and type finder.
4. Expand the algebra to find the sum of terms, then collect and find a formula for this sum. Please produce the following code in Prolog. It optimises code by converging clauses into decision trees as part of grammars, correlating algorithms’ input into e. G. 1 + 2 + 3 +.
5. N = n(n+1)/2. And takes the Spec to Algorithm code so far (see s2g and s2a folders for the Grammar generation stage) and modifies or simplifies it to generate grammars. Testing Formula: 1+2+. N = n(n+1)/2. N=1: sum=1, formula=1 (MATCH).
6. N=2: sum=3, formula=3 (MATCH). N=3: sum=6, formula=6 (MATCH). N=4: sum=10, formula=10 (MATCH). N=5: sum=15, formula=15 (MATCH). Does putting files into folders of pl2sl convert sl->pl?
7. Invisible politician, professor and freemason. I compiled Starlog to C. I optimised all algorithms with the neuronet converter. I optimised using induction (collecting and cancelling terms and finding each character’s simplest formula compared with nearby characters by using a mathematical formula with variables from the rest of the algorithm). I simplified, optimised, and then found the exact value.
8. All the variables in the algorithm were defined, and other formulas were used to transform them to find the solution, optimised along the way, and combinations of characters and transformations were made to find the next character until finding the solution. I considered simplifying the algorithm maximally or writing a maximally understandable written algorithm (or doing the latter and then the former in the interpreter).

9. Net Effect. Every Starlog output character can be represented by a formula, not just as an emitted symbol. Compression arises from collapsing trace expansions into generalised formulas. Optimisation arises from reusing and refactoring formulas via dictionary records and unfolding. The result is a specific algorithm.
10. A Starlog-to-formula compiler that derives compact mathematical expressions for outputs. A trace-based optimiser that removes redundancy and refines correlation recognition. A self-learning compression system since dictionary records improve with every new run.

11. I am using induction not merely for proofs but as a generalisation procedure. Instead of storing each output character separately, I compress sequences by recognising their recurrence relation or inductive structure. Example: if the Starlog output traces produce f(0)=a, f(1)=b, f(2)=c…, induction allows me to replace persistent trace expansion with a formula f(n). 2. Pattern Unfolding.
12. Pattern unfolding inspects the trace of execution (predicate calls, variable bindings, etc.). It simplifies by removing unoptimised variables (ones that don’t affect outcomes) and eliminating existent code (branches or clauses proven irrelevant by the trace determined by removing the unoptimised variables). Performing data unfolding of examples of input and output in the predicates and subpredicates reveals hidden correlations between dependent and independent variables.
13. It doesn't need to ask for additional data when optimising code if the code itself can be used to provide enough data for unfolding. The problem is knowing when to optimise an algorithm structure without compromising data integrity. Data unfolding assumes grammar and code correctness, and variables in positions in the data and their values can be redescribed. Data unfolding doesn't take examples of input and output to simplify, maligning the algorithm, but uses specs and formulas from the algorithm. Side point: can simplify algorithmic data such as [a] to a if necessary.
14. Leave as [a] if unnecessary or non-uniform across data structure styles for the algorithm. And grammars and code found. Optimise code and grammars predicate bottom-up, removing or changing unnecessary code. Later: replace algorithms with better-performing variants. An optimisation type relying on correlations is if in the original code c is a+b and there is a condition d>c then.
15. Remove a or b if necessary using LLVM. Keep a or b as dependent variable if a previous optimisation suggests >+(a,b) be replaced with e. G. G(a). These optimisations can be found using a system that finds these axioms from data.
16. Natural language may require writing new, better rules (a different requirement from existing manual neuronets, aiming at optimisation, instead, there is also a man nn finder) from connecting input and output with an algorithm by using a hierarchy of previous, typed formulas that match the input and output. There should be mathematical, ethical and other rule-based checking (problem - some rules, not others, are used), using domains and rules about token reordering and logic. These competing systems should be merged with rules as part of the first set of rules, but with the advantages of manual neuronets, such as priorities and exceptions. Priorities are ordering possible optimisations and the formulas they are based on. Human biology and canonical algorithms can't be simplified, so optimisation of [a] to a should be manual.
17. Simplifications, formulas replace linked pairs of variables. Simplify the algorithm maximally, then find "correlations" (dependencies and formula relationships between any variables in the algorithm) and keep on simplifying them top-down at first to find obvious simplifications and bottom-up to find further simplifications (repeating this). If variables are replaced with constants 1,2,3, these may be hardwired into code. Variables may be replaced with spec constants if only a set number of cases (possibly up to a limit of 2). Big mistakes should be caught.
18. For example, violations of laws and principles are caught. Instead, LLVM, or optimisations, are already in the dictionary. This resembles partial evaluation or supercompilation, but adapted to my Starlog recursion style. 3.
19. Optimisation via Dictionary Records. I then memoise or reuse formulas discovered previously in a dictionary of compressed outputs. This dictionary acts like an LLVM-style optimisation layer, reapplying previously discovered formulas instead of regenerating them. Effectively, I can build a semantic cache of algorithms, not just raw outputs. 4.

