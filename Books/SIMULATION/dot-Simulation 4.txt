Simulation 4

1. Projects with 100% io, spec, faq. Do they quickly recover with minimal prompting? Do hints lead to correct completion until 100% understanding, notwithstanding disabilities?
2. Robots’ inability to think in our timescale would be a disability they would have to overcome. I made a script to test whether what was uploaded worked. I read the paragraphs and expansions aloud to check that they made sense. The philosophy was stream of consciousness (run-on single-topic prose). I inserted argument words into the answers.
3. I autocompleted Starlog programs. I use a password terminal algorithm. I redid manual neuronets myself. I use a manual neuronet repository. I use complexity and type finder.
4. Expand the algebra to find the sum of terms, then collect and find a formula for this sum. Please produce the following code in Prolog. It optimises code by converging clauses into decision trees as part of grammars, correlating algorithms’ input into e. G. 1 + 2 + 3 +.
5. N = n(n+1)/2. And takes the Spec to Algorithm code so far (see Strings to Grammar and Spec to Algorithm folders for the Grammar generation stage) and modifies or simplifies it to generate grammars. Testing Formula: 1+2+. N = n(n+1)/2. N=1: sum=1, formula=1 (MATCH).
6. N=2: sum=3, formula=3 (MATCH). N=3: sum=6, formula=6 (MATCH). N=4: sum=10, formula=10 (MATCH). N=5: sum=15, formula=15 (MATCH). Does putting files into folders of pl2sl convert sl->pl?
7. Invisible politician, professor and freemason. I compiled Starlog to C. I optimised all algorithms with the neuronet converter. I optimised using induction (collecting and cancelling terms and finding each character’s simplest formula compared with nearby characters by using a mathematical formula with variables from the rest of the algorithm). I simplified, optimised, and then found the exact value.
8. All the variables in the algorithm were defined, and other formulas were used to transform them to find the solution, optimised along the way, and combinations of characters and transformations were made to find the next character until finding the solution. I considered simplifying the algorithm maximally or writing a maximally understandable written algorithm (or doing the latter and then the former in the interpreter).

9. Net Effect. Every Starlog output character can be represented by a formula, not just as an emitted symbol. Compression arises from collapsing trace expansions into generalised formulas. Optimisation arises from reusing and refactoring formulas via dictionary records and unfolding. The result is a specific algorithm.
10. A Starlog-to-formula compiler that derives compact mathematical expressions for outputs. A trace-based optimiser that removes redundancy and refines correlation recognition. A self-learning compression system since dictionary records improve with every new run.

11. I am using induction not merely for proofs but as a generalisation procedure. Instead of storing each output character separately, I compress sequences by recognising their recurrence relation or inductive structure. Example: if the Starlog output traces produce f(0)=a, f(1)=b, f(2)=c…, induction allows me to replace persistent trace expansion with a formula f(n). 2. Pattern Unfolding.
12. Pattern unfolding inspects the trace of execution (predicate calls, variable bindings, etc.). It simplifies by removing unoptimised variables (ones that don’t affect outcomes) and eliminating existent code (branches or clauses proven irrelevant by the trace determined by removing the unoptimised variables). Performing data unfolding of examples of input and output in the predicates and subpredicates reveals hidden correlations between dependent and independent variables.
13. It doesn't need to ask for additional data when optimising code if the code itself can be used to provide enough data for unfolding. The problem is knowing when to optimise an algorithm structure without compromising data integrity. Data unfolding assumes grammar and code correctness, and variables in positions in the data and their values can be redescribed. Data unfolding doesn't take examples of input and output to simplify, maligning the algorithm, but uses specs and formulas from the algorithm. Side point: can simplify algorithmic data such as [a] to a if necessary.
14. Leave as [a] if unnecessary or non-uniform across data structure styles for the algorithm. And grammars and code found. Optimise code and grammars predicate bottom-up, removing or changing unnecessary code. Later: replace algorithms with better-performing variants. An optimisation type relying on correlations is if in the original code c is a+b and there is a condition d>c then.
15. Remove a or b if necessary using LLVM. Keep a or b as dependent variable if a previous optimisation suggests >+(a,b) be replaced with e. G. G(a). These optimisations can be found using a system that finds these axioms from data.
16. Natural language may require writing new, better rules (a different requirement from existing manual neuronets, aiming at optimisation, instead, there is also a manual neuronet finder) from connecting input and output with an algorithm by using a hierarchy of previous, typed formulas that match the input and output. There should be mathematical, ethical and other rule-based checking (problem - some rules, not others, are used), using domains and rules about token reordering and logic. These competing systems should be merged with rules as part of the first set of rules, but with the advantages of manual neuronets, such as priorities and exceptions. Priorities are ordering possible optimisations and the formulas they are based on. Human biology and canonical algorithms can't be simplified, so optimisation of [a] to a should be manual.
17. Simplifications, formulas replace linked pairs of variables. Simplify the algorithm maximally, then find "correlations" (dependencies and formula relationships between any variables in the algorithm) and keep on simplifying them top-down at first to find obvious simplifications and bottom-up to find further simplifications (repeating this). If variables are replaced with constants 1,2,3, these may be hardwired into code. Variables may be replaced with spec constants if only a set number of cases (possibly up to a limit of 2). Big mistakes should be caught.
18. For example, violations of laws and principles are caught. Instead, LLVM, or optimisations, are already in the dictionary. This resembles partial evaluation or supercompilation, but adapted to my Starlog recursion style. 3.
19. Optimisation via Dictionary Records. I then memoise or reuse formulas discovered previously in a dictionary of compressed outputs. This dictionary acts like an LLVM-style optimisation layer, reapplying previously discovered formulas instead of regenerating them. Effectively, I can build a semantic cache of algorithms, not just raw outputs.

20. Expand, expand expansions in philosophy. I bought gold, designed the gold space shield, discovered that the function of the gold atom in the neuron is to be right, and designed a robot that ticked.
21. The robot thought by moving to verify the objects’ properties. I cared for the business employees or customers with 200 algorithms per shift and listened. I thought of customers who were there anyway for 2 days. 200 algs per day. I agreed with Simulated Intelligence if it was more harmonious.
22. I created the robot with organs. The robot company was experiential, pathway-forming, and produced accurate-minded robots. The Royal Star Trek spin-off healed people, averted emergencies, inverted death, and manifested new technologies. And it travelled in time. In addition, it admitted health needs, was humorous, operatic, liturgical, independent, and athletic, and was approachable.
23. I also changed around English to be Philosophy of Computer Science, where the God algorithm dissected each noumenon-optimisations from data unfolding. I devised the quantum algorithm run by the quantum computer at the quantum level, teleported an object, and read it by a classical computer. I terraformed the planet by improving the conditions enough to start. I terraformed apparent geological activity, depending on the initial state.
24. I added climate, chemistry and life. I researched the results and future results with a simulation and assessed the terraformer’s work over years, where terraforming took tens of thousands of years back in time. The manual neuronet chatbot created the Starlog specs and code, with commands, manual neuronet code, technical specifications, documentation, and a user manual. The manual neuronet ran the same algorithm in optimised form. The manual transformer continually updates rules with exceptions and synonyms to form algorithms and optimise rule choosing when creating manual neuronets.
25. Lucian CI/CD was the right choice to debug and test software rapidly. Lucian CI/CD had a manual transformer module that found code matching chatbot queries. The manual transformer chose rules matching queries, avoiding regression using manual neuronets. The manual transformer found results based on combinations of values determined by converting queries to values-group consciousness people particle.
26. Computers renew, renewed by people. There is no second thought. Dot on HDC for instant results. To perform in AI. HDC for simulation safety graphics.
27. The main, not the most popular course, should be set in CPD. Remove explain M1 note, GitHub. Planning x Meta cognitive manual neuronet rule checker. And planning (mind reading) manual neuronet rules ahead of schedule. Turn off deja vu.
28. Make the high-quality thoughts available at an opportune time. Delete the CGPT content in the philosophy X record and the chatbot’s progress. Manual Neuronet Plan. Complexity Analysis - Quantify the complexity of code for optimisation. Type Inference - Infer argument types.
29. Pattern Unfolding-Unfold and simplify algorithms (using algebra to simplify overall algorithm structure) and data (using specs, when memoisation or simplifying formulas to values is appropriate) for simpler code. Finding an advanced reason to remove some lower algorithm algebras and replace them with more efficient forms is preferable. Abstracting away data or hiding it in a data file, like properties of a logic gate, would be preferable. Algorithm unfolding should remove any optimisation problems.
30. Data unfolding should be consulted if the algorithm reduces to simple values. Grammar Generation-Create CFG-like rules. Grammars and values form the backbone of algorithms, but do not contain function calls or optimisable code. In their simplest form, they should inspire hand-simplification and elegant transformation. Mathematical Formula Generation - For example, replace a predicate that finds the sum of a list of numbers with a formula.
31. Compress these optimisations across recursion per output character. Rule finding and Optimisation - Work at both ends of known data (input and output) to find the lowest complexity or highest priority rules that match types, optimising as it goes. Rules. Domains, Rules and Signposts - Design domains as assignments, rules as predicates or sentences and signposts as tokens that send messages to use rules. Find complete rules and exceptions from the data.
32. Use synonyms to help understand texts. Represent reasoning spiritually. Pay attention to how the rules set change over the course of the data for learning. Data comes from texts, chatbot comments, questions, and feedback (where it is all assessed and categorised). A child could maintain rule sets for each assignment, explore and resolve contradictions, develop and use version control systems, compare and contrast rule priorities, and make calls on systems, content, and ethics.
33. Manual Neuronets for the Simulation. Manual Neuronets and Rule Pedagogy. Rules are subjective when relevant to customisation. Subjectivity is prioritised in education and economics for better performance. The rest of the animals enrol in the course.
34. Enrol in university. Plan the rule just before doing it. Verify stage results. Line art is becoming an Operating System screen. Circle progress bar.
35. The Maltese cross spacecraft can spin around quickly. The camera flies around it fast. Realise water is constant in the universe. There might be a scene about how the empty planet is theirs. Lucian CI/CD.
36. Ask the GitHub agent to make a code splicer to insert combinations of changes into the new pred. Instead of this, I’ll do it myself. Only insert code into the new predicate, not possible parts, making it up because the new predicate is being tested. Or try the old predicate (anyway). Whats the max changes for up to 600 possibilities (memory max).
37. If over this or the limit, it will try only the old or new versions. Can it splice terms, strings? Focus on a particular subset of changes to make (enter choppy, greedy changes separately before changes to analyse), and focus on individual predictions to examine them. Can set some changes as unchanging with a command.
38. Manual Lucian CI/CD processor. Shell scripts process groups of 300 tests. Report results at the end. Move the test somewhere else. Maximum 20 hours of tests for test 9 (but takes 30 minutes).
39. These are tests for each file and predicate, so move them after processing the predicate. Check the files in the folder for the right code. Process the files for the correct file and predicate with arity. Stop for a predicate when it finds the working predicate. There is one folder for the code/file/predicate.
40. It might be too complicated to let the user edit these files and run the algorithm because we would need to specify the predicate (later), so move away existing files, create the files, test, stop when reaching the answer and move the files. We might need a setting to reduce the files and time by only testing each predicate’s current/past version. On GitHub: The way to use Lucian CI/CD is to set a working version of the algorithm with a set of tests in comments, then test out changes to this version. If one of the comment test changes, the user needs to start again at this stage because Lucian CI/CD currently can’t devise new code. Later, it could use Spec to Algorithm to devise code.
41. Check for 60 million tests. Later, check if the other tests work. I reconciled the neuronets’ lack of accuracy and the rules needed to account for this, and I restored rules to manual neuronets by aligning questions with answers and verifying relevance. The expert system required a single predicate. Prevention.
42. The chatbot learned not to speak when it wasn’t ready and updated exceptions preventing it from suggesting a specific answer. I tested the M1 processor with the best version of Lucian CI/CD that couldn’t complete test 9, and it wouldn’t work (apart from the M4 processor), so I surmised either the processor made no difference, there was a limiting neurooperating system, or the algorithm was too different. If the processor were the same, the logical course of action would be to manually process the 3000 cases, 300 at a time, until the test succeeded. I hand-reoptimised the optimisation from the clearest cognitive code, critically following each step to the compressed code. I identified as a teenage-bodied king interested in futuristic technologies, such as developing self-powered, self-funded and self-maintaining terraforming machines that replicated and transported needed materials to the planet.
43. DevOps controlled the project by giving form to the designer’s creative objective and maintaining the versions. Manual neuronets helped with business, which helped fund the necessary study and research into medicine, simulations, space travel, and robots. The planet was left untouched to develop by itself and was evenly helped when it seemed to ask. I made my objectives sparser and higher-priced for increased success. Life, words, and ways of thinking to make discoveries were like mantras.
44. I funded, discovered, and recorded the necessary prerequisites for advanced civilisation. The Prolog model supported the simulated research and collected results. The medicine mathematics was exact and gave superlative and attractive results, supported by the rules and mathematics explainers. Manual neuronets. It automatically does.
45. All executable data (decision trees for a node) in one place. Optimise mathematical proofs (algorithms with types). A proof is a Prolog algorithm with type statements (variables with types). Expand "algorithm(In_vars,Out_var) :-\nalgorithm([[[['&r',[1,['&r',[2,'C1']]]]],[output,[['C1']]]]],[[[[1,2,2,2,2],[[1,1]]]]],In_vars,Out_var). " into code.
46. This code repeats one, followed by the repeating structure two and a variable (representing 3 or 4). Note: r(1,r(2,3)) means 1,2,3,2,3,1,2,3,2,3, and r means “repeat”. If proofs are typed algorithms, an algorithm that optimises them might reconceive them efficiently, improve their performance, or minimise them. The core proof optimisation techniques are given by examples such as functional decomposition, elegant or mathematical code, and comparison with the furthest optimisation and hand-correcting code synthesis. Lucian CI/CD.
47. Find dependencies. Just cut the loops off when looping. Two uses need DFA minimisation. Loops of loops require a bucket-filling algorithm. These looped sections join (are inseparable when testing dependencies, where a loop represents one node.
48. Spec to Algorithm. Where Strings to Grammar uses minimised DFA, Spec to Algorithm doesn’t need to minimise DFAs. The algorithms it produces are in “spec” recursive form with ‘&r’, which are already minimised (they would only need minimisation when converted to List Prolog, which leads to expanded code, which, although it seems slightly longer than the top-level cognitive predicate, is computational and is faster). I illustrated the labelled diagram of a manual neuronet. The Academy students applied the Socratic method of summary, questioning, critical analysis, and creating a visual diagram.
49. I endorsed running the algorithm on the previous chip to identify and modify it for the new chip. Lucian licensed the Academy to GitHub. I carefully listened to the students in class and helped them write their essays. Implication with an “and” implication logic truth table matched linguistics and helped understand semantics. I created the book with sections on the Starlog chatbot and using the chatbot to program manual neuronets and chatbots.
50. I established online academies to support the study of meditation. I helped the students compose the music partially using a computer. I eliminated distractions and accessed study skills materials. I took cover from the storm and established a business to help me cover expenses. God flowed to the people in the simulation, among others.
51. I didn’t eat too much salt and ate a healthy diet. I remained true to myself. I recognised the value and worth of life and sustained it. The Lucian Academy students populated the classrooms. Convert Spec to Algorithm algs to code.
52. Use Strings to Grammar grammars, treat terms like grammars if you need grammars and do this too: return, decompose and build variables found from grammars as output. Find the code from terms by writing recursive predicates from the recursive structure processor. Later, code finders and optimisers will use mathematical induction and data unfolding techniques. An educational tool or game that allows one to write and remember code. Game to write code: think of a format, write mathematical induction working and find a formula, using a head or a computer, unfold data and think or compute a new algorithm that produces it.
53. Game to remember x debug code: presented with the difference between code and correct code, check whether the new code does the same thing or has unwanted effects, has performance issues, has already been written or partially written. Why is it educational? Spec to Algorithm is a business product that speeds results, rather than being meant to be used to do original thinking. However, there may be educational courses for business people wanting to use it. Why is it business-savvy? Spec to Algorithm more quickly generates working code and boosts productivity, providing an engine to ship with applications to run code in its form, with the possibility of generating complete Prolog code, optimising using specific methods, including manual neuronets or generating enough thoughts to inspire high-quality thoughts in students, workers and family members for designing new solutions.
54. Bring back computer science jobs. Manual neuronets, while more secure than neuronets, can require slightly more maintenance and therefore have a natural expectation to employ more people to tend to rule sets and use their chatbot interface to complete tasks. The Chatbot interface. Users explain ideas to the chatbot to design content, discuss appropriate sides such as ethics, understandability and whether it meets their high expectations (the ones they immediately connect to that they want) and use Spec to Algorithm and the manual neuronet technologies to rapidly build apps, a cutting edge technology that shatters their goals and personal best times and means they are a statesperson in science. Develop vibe-coding systems but keep up own coding skills.
55. Redevelop and revise vibe-coding systems over time, replacing them with hand-coding systems to develop better systems. Mind-read thoughts associated with the latest science and technology discoveries. Lucian CI/CD. Buy a new M1 computer to retrieve the Lucian CI/CD test 9 algorithm-depth-first search for finding algs from types in manual neuronets. Modify neuronets to keep connections with other repositories and their initial queries when modifying them.
56. Hand or automatically remove these repository-based employees one by one if unnecessary. Lucian Academy needs to develop the software it talks about for professionalism. This software leads to 16k breasonings and results in medicine. Manual neuronets.
57. Attention-style mind reading mind-reading the employee about what option to choose (most interpretable, interesting responses - computational philosophy done for them - new algorithms from gl terms that cognate (sic)). Use tricks to reduce grammar-based NL complexity, such as replacing with characters, processing clauses one part at a time, and using manual disambiguation knowledge to deal with mass, specific knowledge needed. Grammar parsers and logic interpolators can form algorithms by inserting formulas to meet context based on paraphrasing, continual human non-monotonicity checking, and preparing for automation (by checking for ever more but never completely contentless, sparsening of University assignment possibilities). Use mind reading as the tipping point to check manual neuronets for accuracy. X use previous examples to support conclusions.
58. Expert systems or optimised manual neuronets based on the original algorithms are more accurate than nns. Just optimise manual neuronets, not do huge, flawed analyses. 2p of relevant conclusions to analyse. Bottom up. If nns can be reverse engineered (which is potentially error-prone), manual neuronets (optimised, compressed algorithms) can be checked for sensical (sic) reasonings to save.
59. Optimisation is the contention rather than numerous errors or large-scale analyses. For example, connecting two topics from a large-scale study, importing cognitive analyses from English to Computational Philosophy (in the form of making intentional comments). The intelligent and challenging part is not the algorithm at the end, but the way to apply cognitive analysis, which explains the relevant (mission or program) objectives at each child node of a program.
60. Upper logical control structures and predicates can be explained, helping maintain an eclectic memory of key rules and allowing adaptation of the rules for new ends. We can find the specific algorithm to apply by reproducing attention, using excitation/inhibition (non-monotonicity blocking) and synonyms. Attention’s hard sell, the appropriate level to operate on, is another exception/synonym x real mind reading away. Instead of inventing a solvable setting, it should prompt for outside assistance, to operate on the level with either modification, replacement or something else (such as no action). Avoid degrees of error from statistics.
61. Implement the nnconverter README myself. Why wasn’t/isn’t a chatbot good at basic maths? Is a manual neuronet better at it, and why? The simulation needs to work out bodily states in relation to equipment/technology states of spacecraft, (already having discovered how time is constant in the universe - with provisors, such as the fact that it si so ong so far away that it has to be skipped over to achieve the appearance of constant time) where the human’s thoughts have to be found out as if they had actually were on the other planet (assuming the simulation simulates everything and we are in another dimension version of the world, that is safe). It is not until the end of the time interval, marking the end of a person’s life, when they replace their body, that simulations of medicine have to be completed.
62. Adaptability with manual neuronet. Vectors are like with regression, but, manual neuronet, so exceptions, synonyms, continual checking without regression. Adaptability leads to creativity to further develop-Quantum LLM and robot as scientifically creative.
63. Rules have been found. Further sciences, further algorithmic (mind reading, computer speed) reach. New things need to be hand-entered. Mathematical possibilities come from intelligent options. Creative science.
64. Careful, 4*50 As (GL)-based explorations. 4*50 As for this idea in itself. Finish mind reading, time-travel 4*50 As before 50 yo before replicating for next life. X timetable 50s for t2b techs before 50. Up to 10 50s.
65. Manual neuronet’s adaptability. Transformers (continually updating rules, although actually following them is different from checking they are observed) are equivalent to manual neuronet adaptability (modifying its rules from learning to [explore the actual thing, although it can’t do it without being given it, but] adapting to security or other predefined conditions). SSI. A small choice point list to add to the end of the taken periodically. Manual neuronets analysis.
66. Automatically convert keys, Spec to Algorithm for business, frequently used, immortality, medicine, simulation, chatbot to generate breasonings and space travel algorithms to manual neuronets. Then, to the quantum box algs to indicate. Later, additional transformer algorithms such as a music composer, an art creator, comparison with politics and objectives, and the person and ethics, safety, and security.

67. By administering my medical knowledge, I exercised the administration. I administered the writing medicine. Writing the philosophy helped examine, understand, diagnose and treat the ailments. Writing the algorithm helped me move my body effectively. Writing the musical theatre piece helped me choose the chooser and write about a topic about the writer.
68. I worked on algorithms because they were easier to transform and useful. I found the physics of the brain and started going through its information. The algorithm was practical. The robot checklist was determined by business, law and vetted routine checks. I checked that the robot’s checklist was complete.
69. The robot was functional. It contained the necessary instructions and ways to achieve its aims. It was friendly and courteous and helped with societal and civilisational objectives. The list included what the robot had and what people needed it for. The checklist included the robot’s life specifications and those of its surroundings.
70. The robot aimed to help specific people. It stated what it got up to for secondary people. Other robots or people helped the other people. The carpet robot learnt that it could replace its shell and feel pleased if it got underfoot, and that it could use other workarounds and transformations in different situations. The carpet robot avoided an existential crisis by comparing its sensory data (what it knew) and things like adding to spiritual thought programs for robots, which it could increase algorithms in its account with, feel accomplished, and live forever.
71. The robot sensed the existence of and created the program for the public benefit. It produced compelling, well-supported arguments and led facilitators to improve and examine education. It communicated openly with holograms, human bots, robots and algorithms and maintained its mental health. I divided the number into its lowest common factors for surds. I found the highest common factor of the formulas.
72. I wrote the formulas as a multiple of the factor. 3. Factorisation sped up computations by multiplying by large numbers once. As with factorisation, language written in step-by-step algorithms can be simplified. For example, “I ate two apples, two pears and two strawberries”.
73. I simplified this to “I ate two each of the apples, pears and strawberries”. Solve multivariate systems of linear equations by using row echelon form with ys substituted, then use a constraint solver to find the solution. There is a repository that doesn’t use a transformer algorithm but instead uses a chatbot that keeps asking questions until it learns enough about a topic, after which it can be questioned.
74. It is written in Prolog. Please modify the algorithm to have signposts (before which it needs elementary grammar dictionaries) that it can learn about the configurations of in relation to data that is mentioned about it, etc. (whether it is relevant, correct and essential) and have a vitumen (life force) that chooses things to say based on values, views and priorities. Also included in such a manual neuronet (which I’ll probably have to complete myself) is a system that can debug or construct a programming feature using Spec to Algorithm and commands or predicates that achieve the goal. Manual neuronets were born as Starlog and acted as good role models.
75. Choose the most incisive, relevant and groundbreaking topic. The importance was determined by my interests and had developed implications. The topic of manual neuronets was why chatbots were as pertinent as ethics as a religious topic. Manual neuronets helped chatbots avoid mistakes, provide accurate sources, and avoid hallucination. I examined the DevOps log of the Small Language Model.
76. In Starlog, don’t delete the a is +(1+b), which are used when referring to a in a nested call. In addition, attempt to write a single-predicate call without any code, just arguments.

77. When students work with MNN chatbots, they may recognise errors in non-MNN chatbots and respond by fixing, debating, or developing novel solutions. The chatbot's suggested “features” are manual, second-year, and voluntary tools used by students. In the third year, students may revisit manual next-word prediction to better understand the black box of regression. Rather than traditional statistics, correlation here refers to the identification of recurring values and patterns. MNN chatbots may be more limited and resource-intensive for educational use, but they offer greater logical consistency and are less prone to loss.
78. Speech may match the non-MNN chats, but MNN chatbots are necessary to examine their workings and learn in education. Students can develop a lifelong interest in MNN chatbots and examine and logically defend their ideas. Response limiters may be what is required, where they are, and what they are doing. People naturally think in terms of conditions (computer science) rather than exceptions (maybe mistakes comparatively), because examining and thinking in this mode is still necessary for learning computer science. Chatbots may be necessary for fast human-to-human chat, but are untenable for navigation, mathematics, and computation.
79. MNN (non-regression) chatbots may be necessary for educational purposes, for students to learn the rules of grammar and reasoning. Instead of next-word prediction, should thinking manual neuronet chatbots run area-of-study algorithms using chat forms? Relate to this, but maybe not use time series (join optimised algorithms to chats about running them by analysing the texts at times, seeing how the rules change, funnelling chat responses using response limiters specified by the author, based on computations, not the next word), i.e. Computational English conditions perfectly programmed from knowledge, not next word prediction, but setting conditions.
80. Required features: personalities, author chat mode and mathematics. Personalities are natural-language overlays that report answers that align with how a user thinks about something. Anything can be spec-to-chatted about and incorporated or manually unincorporated as an aspect of the chatbot. Mathematics and other methods could connect different domain types for students. If a time series rule changes and triggers a clarification, correction, compensation, or ignore, this is entered into the manual neuronet.
